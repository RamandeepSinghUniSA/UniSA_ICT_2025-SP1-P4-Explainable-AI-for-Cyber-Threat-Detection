{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the UNSW-NB15 Dataset\n",
    "\n",
    "In this part, the preprocessing of the dataset is performed. The steps followed in this part are as follows:\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Loading the Dataset**\n",
    "   - Load the UNSW-NB15 dataset from a CSV file into a pandas DataFrame for further processing.\n",
    "   - The code reads a cleaned CSV file ('Cleaned_full_data.csv') into a pandas DataFrame.\n",
    "   - It verifies the shape of the DataFrame to ensure the data is loaded correctly.\n",
    "\n",
    "2. **Encoding Categorical Features**\n",
    "   - Convert categorical features into numerical values using label encoding. This is necessary for machine learning algorithms that require numerical input.\n",
    "\n",
    "3. **Scaling Numerical Features**\n",
    "   - Standardize the numerical features by scaling them to have zero mean and unit variance. This helps in improving the performance of machine learning models.\n",
    "\n",
    "4. **Splitting the Dataset into Training and Testing Sets**\n",
    "   - Split the dataset into training and testing sets to evaluate the performance of machine learning models. Typically, 80% of the data is used for training and 20% for testing.\n",
    "\n",
    "5. **Main Preprocessing Function**\n",
    "   - The preprocess_unsw_nb15 function orchestrates the entire preprocessing pipeline.\n",
    "   - It identifies categorical and numerical columns, applies encoding and scaling, and splits the data.\n",
    "   - It returns the processed training and testing sets, encoders, and scaler.\n",
    "\n",
    "6. **Execusion and Output** \n",
    "   - The if __name__ == \"__main__\": block executes the preprocessing steps.\n",
    "   - It prints the shapes of the training and testing sets, confirming the split.\n",
    "   - It prints the head of the processed training and test data, to show the results of the preprocessing steps.\n",
    "   - cThe processed data is saved to pickle files for later use in model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations and arrays\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Enable inline plotting in Jupyter notebooks\n",
    "# Fixed duplicate import and invalid syntax\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Cleaned Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Reading datasets\n",
    "# Using list comprehension to read all csv files in 4 csv files\n",
    "df = pd.read_csv('C:/Users/raman/OneDrive/Important/1UnisaSTUDY/Courses/Capstone_Project_1/Github/Code Working/Data Cleaning and EDA/Cleaned_full_data.csv', header=0) \n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Full data is avaliable or not\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Encoding Categorical Features**\n",
    "   - Convert categorical features into numerical values using label encoding. This is necessary for machine learning algorithms that require numerical input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode categorical features\n",
    "# Label Encoding: Converts categories to numerical values (0,1,2...). Good for ordinal data but can imply ordering\n",
    "# One-Hot Encoding: Creates binary columns for each category. Better for nominal data with no inherent order\n",
    "def encode_categorical_features(df, categorical_features, encoding_type='label'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        categorical_features: List of categorical column names\n",
    "        encoding_type: 'LabelEncoder' for LabelEncoder or 'onehot' for OneHotEncoder\n",
    "    \"\"\"\n",
    "    if encoding_type == 'LabelEncoder':\n",
    "        label_encoders = {}\n",
    "        for column in categorical_features:\n",
    "            label_encoders[column] = LabelEncoder()\n",
    "            df[column] = label_encoders[column].fit_transform(df[column].astype(str))\n",
    "        return df, label_encoders\n",
    "    \n",
    "    elif encoding_type == 'onehot':\n",
    "        onehot = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        encoded_array = onehot.fit_transform(df[categorical_features])\n",
    "        \n",
    "        # Create new column names for one-hot encoded features\n",
    "        new_columns = []\n",
    "        for i, feature in enumerate(categorical_features):\n",
    "            categories = onehot.categories_[i]\n",
    "            new_columns.extend([f\"{feature}_{cat}\" for cat in categories])\n",
    "            \n",
    "        # Create new dataframe with encoded features\n",
    "        encoded_df = pd.DataFrame(encoded_array, columns=new_columns, index=df.index)\n",
    "        \n",
    "        # Drop original categorical columns and concat encoded ones\n",
    "        df = df.drop(columns=categorical_features)\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "        return df, onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling Numerical Features**\n",
    "   - Standardize the numerical features by scaling them to have zero mean and unit variance. This helps in improving the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale numerical features\n",
    "def scale_numerical_features(df, numerical_features):\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the Dataset into Training and Testing Sets**\n",
    "   - Use `train_test_split` from the `sklearn.model_selection` module to split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset into training and testing sets\n",
    "def split_dataset(df, target_column, test_size=0.2, random_state=42):\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main function to preprocess the dataset\n",
    "def preprocess_unsw_nb15(df, target_column, categorical_features, numerical_features, encoding_type='LabelEncoder'):\n",
    "    df, encoders = encode_categorical_features(df, categorical_features, encoding_type)\n",
    "    df, scaler = scale_numerical_features(df, numerical_features)\n",
    "    X_train, X_test, y_train, y_test = split_dataset(df, target_column)\n",
    "    return X_train, X_test, y_train, y_test, encoders, scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming df is already loaded and categorical and numerical features are identified\n",
    "    target_column = 'label'  # Update with the correct target column\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "\n",
    "    # Preprocessing\n",
    "    encoding_type = 'LabelEncoder'  # Change to 'onehot' for One-Hot Encoding\n",
    "    X_train, X_test, y_train, y_test, encoders, scaler = preprocess_unsw_nb15(df, target_column, categorical_features, numerical_features, encoding_type)\n",
    "    print(\"Preprocessing complete.\")\n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "    print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print head values\n",
    "print(\"\\nTraining set head:\")\n",
    "print(X_train.head())\n",
    "print(y_train.head())\n",
    "\n",
    "print(\"\\nTest set head:\")\n",
    "print(X_test.head())\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "#X_train.to_csv('X_train.csv', index=False)\n",
    "#X_test.to_csv('X_test.csv', index=False)\n",
    "#y_train.to_csv('y_train.csv', index=False)\n",
    "#y_test.to_csv('y_test.csv', index=False)\n",
    "#print(\"CSV files saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle\n",
    "output_folder = 'C:/Users/raman/OneDrive/Important/1UnisaSTUDY/Courses/Capstone_Project_1/Github/Code Working/Pickle'  # Change this to your desired folder\n",
    "\n",
    "# Ensure the output folder exists\n",
    "import os\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_folder, 'X_train.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "with open(os.path.join(output_folder, 'X_test.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "with open(os.path.join(output_folder, 'y_train.pkl'), 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "with open(os.path.join(output_folder, 'y_test.pkl'), 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "print(\"Pickle files saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
