{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shap\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from explainerdashboard import InlineExplainer, ExplainerDashboard, ClassifierExplainer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrEncoder:\n",
    "    \"\"\"\n",
    "    CorrEncoder: Takes a dataset as input and uses it for the encode function. Encodes the filtered categories then draws correlations.\n",
    "    If correlation is above the threshold adds it to a new dataframe then returns the one hot encoded values with the labels.\n",
    "\n",
    "    Initialisation:\n",
    "        - data (pd.DataFrame): The Dataset that contains the target column and target label variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data.reset_index(drop=True).copy()\n",
    "        self.data = self.data.drop(columns=['Label'])\n",
    "\n",
    "    def encode(self, target_column, sparse_n, threshold):\n",
    "        \"\"\"\n",
    "        encode: Takes a target column and target label to encode and draw correlations from. The target column is iterated through\n",
    "        for all categories that contain more positive values than defined in sparse_n. This allows for filtering of sparse categories.\n",
    "        The function then one hot encodes the given category with the static target column and draws correlations for them. If correlation\n",
    "        is greater then threshold then add it to the new DataFrame. The function returns the one hot encoded categories that pass the\n",
    "        threshold with the target label.\n",
    "\n",
    "        The purpose of this function is to resolve the high cardinality problem in one hot encoding.\n",
    "\n",
    "        Parameters:\n",
    "            - target_column (string): The name of the target column. The target column should contain the various categories to encode.\n",
    "            - sparse_n (integer): The minimum amount of positive values required for a category after encoding (deals with sparse categories).\n",
    "            - threshold (float): The threshold for correlation. The function creates onehot encoded columns of all variables that high correlation\n",
    "              higher that the threshold to the target label.\n",
    "            - cat (string): The category label to compare to.\n",
    "\n",
    "        Returns:\n",
    "            - ohe_df (pd.DataFrame): The one hot encoded values from the target columns.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.data[target_column] = self.data[target_column].astype(str)\n",
    "        value_counts = self.data[target_column].value_counts()\n",
    "        # Check if number of 1s is above the given threshold set by sparse_n.\n",
    "        categories = value_counts[value_counts > sparse_n].index.tolist()\n",
    "        ohe_list = []    \n",
    "        attack_cat = self.data['attack_cat']\n",
    "        # Go through each unique category in the target column.\n",
    "        for c in categories:\n",
    "            col_name = f'{target_column}_{c}'\n",
    "\n",
    "            # Create the binary encoding column for the current category and target label\n",
    "            corr_column = (self.data[target_column] == c).astype(int)\n",
    "            correlation = corr_column.corr(attack_cat)\n",
    "\n",
    "            # Check if absolute correlation is greater than threshold.\n",
    "            if abs(correlation) > threshold:\n",
    "                corr_column.name = col_name\n",
    "                ohe_list.append(corr_column)\n",
    "        print('Number of Encoded Features for', target_column)\n",
    "        print(len(ohe_list))\n",
    "        if ohe_list:\n",
    "            # NOTE: This section can be expanded to include print outs but at the moment am focusing on the evaluations.\n",
    "            ohe_df = pd.concat(ohe_list, axis=1)\n",
    "            return ohe_df\n",
    "        else:\n",
    "            # This ommits errors (if really high thresholds are used).\n",
    "            print(\"No correlations exceed the threshold.\")\n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(size, rs, threshold, downsample, split_method, scaler_type, category):\n",
    "    \"\"\"\n",
    "    get_data: Example usage of CorrEncoder and Downsampling.\n",
    "\n",
    "    Parameters:\n",
    "        - size (float): Proportion split for the validation set.\n",
    "        - rs (int): Random seeds.\n",
    "        - threshold (float): Correlation threshold for CorrEncoder().\n",
    "        - downsample (string, float): Either 'full' or a proportion to downsample the label to.\n",
    "        - split_method (string): 'slice' or sample the validation set.\n",
    "        - category (string): The threat category to process data for.\n",
    "    \n",
    "    Returns:\n",
    "        - train_data (pd.DataFrame): The training set.\n",
    "        - val_data (pd.DataFrame): The validation set.\n",
    "    \"\"\"\n",
    "    feature_names = pd.read_csv('../features2.csv')\n",
    "    feature_names_list = feature_names['Name'].tolist()\n",
    "    datasets = []\n",
    "    for i in range(1, 5):\n",
    "        df = pd.read_csv(f'../UNSW-NB15_{i}.csv', header=None)\n",
    "        df.columns = feature_names_list\n",
    "        df.loc[df['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "        datasets.append(df)\n",
    "    filtered_datasets = []\n",
    "    for df in datasets:\n",
    "        length1 = len(df)\n",
    "        df.columns = df.columns.str.replace(' ', '')\n",
    "        df['attack_cat'] = df['attack_cat'].str.replace(r'\\s+', '', regex=True)\n",
    "        df['attack_cat'] = df['attack_cat'].str.replace('Backdoors', 'Backdoor')\n",
    "        df = df[~df['sport'].astype(str).str.startswith('0x')]\n",
    "        df = df[~df['dsport'].astype(str).str.startswith('0x')]\n",
    "        # Does not align with the Features CSV description?\n",
    "        #df = df.drop(columns=['is_ftp_login'])\n",
    "        df['ct_ftp_cmd'] = df['ct_ftp_cmd'].astype(str)\n",
    "        df['ct_ftp_cmd'] = df['ct_ftp_cmd'].replace(' ', 0)\n",
    "        df['ct_ftp_cmd'] = df['ct_ftp_cmd'].astype(int)\n",
    "        df['is_ftp_login'] = df['is_ftp_login'].fillna(0)\n",
    "        df['is_ftp_login'] = np.where(df['is_ftp_login']>1, 1, df['is_ftp_login'])\n",
    "        df['ct_flw_http_mthd'] = df['ct_flw_http_mthd'].fillna(0).astype(int)\n",
    "        print(f\"Filtered Rows (Cleaning): {length1 - len(df)}\")\n",
    "\n",
    "        df['attack_cat'] = (df['attack_cat'] == category).astype(int)\n",
    "        if downsample == 'full':\n",
    "            threat_rows = df[df['attack_cat'] != 'Normal']\n",
    "            num_threat_rows = len(threat_rows)\n",
    "            print(df['attack_cat'].value_counts())\n",
    "            normal_rows = df[df['attack_cat'] == 'Normal']\n",
    "            sampled_data = normal_rows.sample(n=num_threat_rows, random_state=rs)\n",
    "            df = pd.concat([threat_rows, sampled_data]).reset_index(drop=True)\n",
    "\n",
    "        # Downsample by a given pecentage.\n",
    "        elif downsample is not None:\n",
    "            mask = (df['Label'].shift(-1) != 1) & (df['Label'].shift(1) != 1)\n",
    "            normal_rows = df[(df['attack_cat'] == 'Normal') & mask]\n",
    "            percentage_to_remove = int(len(normal_rows) * downsample)\n",
    "            rows_to_remove = normal_rows.sample(n=percentage_to_remove, random_state=rs)\n",
    "            df = df.drop(rows_to_remove.index)\n",
    "            print(f\"Downsampled Rows: {len(rows_to_remove)}\")\n",
    "\n",
    "        filtered_datasets.append(df)\n",
    "    full_data = pd.concat(filtered_datasets).reset_index(drop=True)\n",
    "\n",
    "    # These only have a few unique values so easy to encode.\n",
    "    categorical_columns = ['state', 'service']\n",
    "    encoder = OneHotEncoder(sparse_output=False, dtype='float32')\n",
    "    encoded_data = encoder.fit_transform(full_data[categorical_columns])\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns), index=full_data.index)\n",
    "    full_encoded = pd.concat([full_data.drop(columns=categorical_columns), encoded_df], axis=1)\n",
    "    # Encode categories with many unique values using a threshold.\n",
    "    encoder = CorrEncoder(full_encoded)\n",
    "    ohe1 = encoder.encode('dsport', 30, threshold)\n",
    "    ohe2 = encoder.encode('proto', 30, threshold)\n",
    "    ohe3 = encoder.encode('sport', 30, threshold)\n",
    "    ohe4 = encoder.encode('srcip', 30, threshold)\n",
    "    ohe5 = encoder.encode('dstip', 30, threshold)\n",
    "    cols_to_drop = ['dsport', 'proto', 'sport', 'srcip', 'dstip']\n",
    "    filtered_data = full_encoded.drop(columns=cols_to_drop)\n",
    "    combined_data = pd.concat([filtered_data, ohe1, ohe2, ohe3, ohe4, ohe5], axis=1)\n",
    "    df_features = combined_data.drop(columns=['attack_cat', 'Label'])\n",
    "    if scaler_type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df_features)\n",
    "    final_data = pd.DataFrame(scaled_data, columns=df_features.columns, index=combined_data.index)\n",
    "    final_data['attack_cat'] = combined_data['attack_cat']\n",
    "    if split_method == 'slice':\n",
    "        slice_size = int(size * len(final_data))\n",
    "        val_start = random.randrange(0, len(final_data) - 2 * slice_size)\n",
    "        val_end = val_start + slice_size\n",
    "        val_data = final_data.iloc[val_start:val_end]\n",
    "        train_data = final_data.drop(val_data.index)\n",
    "    elif split_method == 'shuffle':\n",
    "        train_data, val_data = train_test_split(final_data, test_size=size, random_state=rs)\n",
    "    \n",
    "    return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20% Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_28368\\1927573160.py:21: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_28368\\1927573160.py:21: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Rows (Cleaning): 62\n",
      "Downsampled Rows: 0\n",
      "Filtered Rows (Cleaning): 61\n",
      "Downsampled Rows: 0\n",
      "Filtered Rows (Cleaning): 105\n",
      "Downsampled Rows: 0\n",
      "Filtered Rows (Cleaning): 75\n",
      "Downsampled Rows: 0\n",
      "Number of Encoded Features for dsport\n",
      "1\n",
      "Number of Encoded Features for proto\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = get_data(\n",
    "    size=0.2,\n",
    "    rs=42,\n",
    "    threshold=0.1,\n",
    "    downsample=0.99,\n",
    "    split_method='shuffle',\n",
    "    scaler_type='standard',\n",
    "    category='Analysis'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "0    896588\n",
      "1     19443\n",
      "Name: count, dtype: int64\n",
      "attack_cat\n",
      "0    224205\n",
      "1      4803\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['attack_cat'].value_counts())\n",
    "print(val_data['attack_cat'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9956027737022287\n",
      "Precision: 0.8834343434343435\n",
      "Recall: 0.9104726212783677\n",
      "F1 Score: 0.8967497180354763\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    224205\n",
      "           1       0.88      0.91      0.90      4803\n",
      "\n",
      "    accuracy                           1.00    229008\n",
      "   macro avg       0.94      0.95      0.95    229008\n",
      "weighted avg       1.00      1.00      1.00    229008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data.drop(columns=['attack_cat'])\n",
    "y_train = train_data['attack_cat']\n",
    "X_val = val_data.drop(columns=['attack_cat'])\n",
    "y_val = val_data['attack_cat']\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision:\", precision_score(y_val, y_pred))\n",
    "print(\"Recall:\", recall_score(y_val, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90% Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_17628\\1339685965.py:21: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'UNSW-NB15_{i}.csv', header=None)\n",
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_17628\\1339685965.py:21: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'UNSW-NB15_{i}.csv', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Rows (Cleaning): 62\n",
      "Downsampled Rows: 606951\n",
      "Filtered Rows (Cleaning): 61\n",
      "Downsampled Rows: 578493\n",
      "Filtered Rows (Cleaning): 105\n",
      "Downsampled Rows: 479475\n",
      "Filtered Rows (Cleaning): 75\n",
      "Downsampled Rows: 310883\n",
      "Number of Encoded Features for dsport\n",
      "9\n",
      "Number of Encoded Features for proto\n",
      "1\n",
      "Number of Encoded Features for sport\n",
      "2\n",
      "Number of Encoded Features for srcip\n",
      "4\n",
      "Number of Encoded Features for dstip\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = get_data(\n",
    "    size=0.2,\n",
    "    rs=42,\n",
    "    threshold=0.1,\n",
    "    downsample=0.9,\n",
    "    split_method='shuffle',\n",
    "    scaler_type='standard',\n",
    "    category='Analysis'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "0    194117\n",
      "1     19407\n",
      "Name: count, dtype: int64\n",
      "attack_cat\n",
      "0    48542\n",
      "1     4839\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['attack_cat'].value_counts())\n",
    "print(val_data['attack_cat'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9892658436522358\n",
      "Precision: 0.9003378378378378\n",
      "Recall: 0.9913205207687539\n",
      "F1 Score: 0.9436411920920625\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     48542\n",
      "           1       0.90      0.99      0.94      4839\n",
      "\n",
      "    accuracy                           0.99     53381\n",
      "   macro avg       0.95      0.99      0.97     53381\n",
      "weighted avg       0.99      0.99      0.99     53381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data.drop(columns=['attack_cat'])\n",
    "y_train = train_data['attack_cat']\n",
    "X_val = val_data.drop(columns=['attack_cat'])\n",
    "y_val = val_data['attack_cat']\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision:\", precision_score(y_val, y_pred))\n",
    "print(\"Recall:\", recall_score(y_val, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda311new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
