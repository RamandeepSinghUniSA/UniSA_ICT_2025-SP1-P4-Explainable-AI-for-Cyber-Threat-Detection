{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a benchmark comparison for the simplified encoder to see the results in Random Forest and ExplainerDashboard for all threats:\n",
    "- The procedure focuses on gathering the top 30 correlated values to threats of each categorcial variable and provides some information throughout.\n",
    "- The printouts include the highest to lowest correlated values in each category (currently top 30).\n",
    "- Encoders have been optimised with tensors to reduce memory allocation issues.\n",
    "- The following update will include the implementation into ExplainerDashboard by reducing the categories to top10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "from explainerdashboard import ExplainerDashboard, ClassifierExplainer, InlineExplainer\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_504\\3922749488.py:2: DtypeWarning: Columns (1,3) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "# Imports the cleaned dataset.\n",
    "data = pd.read_csv('Cleaned_full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset retained index.\n",
    "data = data.reset_index(drop=True)\n",
    "# Set NA to 0.\n",
    "# Some extra cleaning.\n",
    "data['ct_ftp_cmd'] = data['ct_ftp_cmd'].fillna(0)\n",
    "data['attack_cat'] = data['attack_cat'].str.replace(r'\\s+', '', regex=True)\n",
    "data['attack_cat'] = data['attack_cat'].str.replace('Backdoors', 'Backdoor')\n",
    "# Drop correlated features 99% +.\n",
    "data = data.drop(columns=['dwin', 'dloss'])\n",
    "# Normal rows with hidden strings.\n",
    "data = data[~data['sport'].str.startswith(('0x', '-'), na=False)]\n",
    "data = data[~data['dsport'].str.startswith(('0x', '-'), na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            59.166.0\n",
      "1            59.166.0\n",
      "2            59.166.0\n",
      "3            59.166.0\n",
      "4            59.166.0\n",
      "              ...    \n",
      "2540042      59.166.0\n",
      "2540043      59.166.0\n",
      "2540044      59.166.0\n",
      "2540045      59.166.0\n",
      "2540046    175.45.176\n",
      "Name: srcip, Length: 2539739, dtype: object\n",
      "8\n",
      "srcip\n",
      "59.166.0       1942302\n",
      "175.45.176      361690\n",
      "149.171.126     217315\n",
      "10.40.182         9581\n",
      "10.40.85          6702\n",
      "10.40.170         2094\n",
      "192.168.241         54\n",
      "127.0.0              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Technically we can group the IP addresses directly to a router disregarding individual devices. This is done by removing the last value and dot point.\n",
    "# Can ask the stakeholder which might be better but at the moment we are not facing issues in cardinality with these either.\n",
    "# NOTE: A similar thing can be done with ports to set the to well known, common, and private ports but this might lose a lot of information.\n",
    "# By collecting the ports and ips from the training set it becomes more relevant to the data.\n",
    "# Some websites also offer lists of vulnerable ports but again this might not be relative to the data we have to train the model.\n",
    "# We can assume that the ports correlated to threats are commonly used anyway.\n",
    "# Wikipedia offers a list of ports which we can get information from: https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers.\n",
    "# I will create a selenium scraper that can make a dataframe we can link ports to descriptions.\n",
    "grouped_ip = data['srcip']\n",
    "print(len(grouped_ip.value_counts()))\n",
    "print(grouped_ip.value_counts())\n",
    "grouped_ip = grouped_ip.str.rsplit('.', n=1).str[0]\n",
    "print(len(grouped_ip.value_counts()))\n",
    "print(grouped_ip.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrOnehotEncoder:\n",
    "    \"\"\"\n",
    "    CorrOnehotEncoder: Encodes the given column by creating one-hot encoded columns for categories that have\n",
    "    a correlation higher than a threshold with the target column.\n",
    "    \"\"\"\n",
    "    def __init__(self, column, target):\n",
    "        \"\"\"\n",
    "        Constructor: Stores the column and target (storing the full data causes memory issues).\n",
    "        \n",
    "        Parameters:\n",
    "            - column (pd.Series): The feature column to encode.\n",
    "            - target (pd.Series): The target column.\n",
    "        \"\"\"\n",
    "        # Force to string for groups.\n",
    "        self.column = column.astype(str)\n",
    "        # Convert to float32 precision to minimise memory load.\n",
    "        self.target = target.astype(np.float32)\n",
    "\n",
    "    def corr(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculate the Pearson correlation coefficient (Phi).\n",
    "        \n",
    "        Parameters:\n",
    "            - x (tensor - float32): The first variable.\n",
    "            - y (tensor - float32): The target to draw correlation to.\n",
    "        \n",
    "        Returns:\n",
    "            - r (float32): The Pearson correlation coefficient (Phi).\n",
    "        \"\"\"\n",
    "        mean_x = tf.reduce_mean(x)\n",
    "        mean_y = tf.reduce_mean(y)\n",
    "        covariance = tf.reduce_sum((x - mean_x) * (y - mean_y))\n",
    "        std_x = tf.sqrt(tf.reduce_sum((x - mean_x) ** 2))\n",
    "        std_y = tf.sqrt(tf.reduce_sum((y - mean_y) ** 2))\n",
    "        r = covariance / (std_x * std_y)\n",
    "        return r\n",
    "\n",
    "    def encode(self, sparse_n, threshold, max_encoded):\n",
    "        \"\"\"\n",
    "        Encode the feature column by creating one-hot encoded columns for categories that have\n",
    "        a correlation higher than a threshold with the target.\n",
    "        \n",
    "        Parameters:\n",
    "            - sparse_n (int): Minimum number of occurrences (1's) for a category in the column.\n",
    "            - threshold (float): The correlation threshold.\n",
    "            - max_encoded (int): The maximum number of encoded features.\n",
    "        \n",
    "        Returns:\n",
    "            - ohe_df (pd.DataFrame): One-hot encoded columns that meet the correlation threshold.\n",
    "        \"\"\"\n",
    "        # Convert to numpy for tensors.\n",
    "        column_np = self.column.to_numpy()\n",
    "        target_np = self.target.to_numpy()\n",
    "\n",
    "        # Store results.\n",
    "        ohe_list = []    \n",
    "        column_names = []\n",
    "        correlations = []\n",
    "        # Iterate through each unique category in the column.\n",
    "        for c in np.unique(column_np):\n",
    "            # Convert to binary - float32 minimises memory issues.\n",
    "            corr_column = (column_np == c).astype(np.float32)\n",
    "            # If the category count is below sparse_n, skip encoding.\n",
    "            if np.sum(corr_column) < sparse_n:\n",
    "                continue\n",
    "            # Convert to tensors for the correlation calculation.\n",
    "            correlation = self.corr(tf.convert_to_tensor(corr_column, dtype=tf.float32), \n",
    "                                    tf.convert_to_tensor(target_np, dtype=tf.float32))\n",
    "            # If the absolute correlation is greater than the threshold, add to the list.\n",
    "            if abs(correlation.numpy()) > threshold:\n",
    "                ohe_list.append(corr_column)\n",
    "                column_names.append(c)\n",
    "                # Store correlations to sort.\n",
    "                correlations.append(abs(correlation.numpy()))\n",
    "\n",
    "        # Sort the columns by their correlation with the target.\n",
    "        sorted_indices = np.argsort(correlations)[::-1]\n",
    "        sorted_ohe_list = []\n",
    "        sorted_column_names = []\n",
    "        for i in sorted_indices:\n",
    "            sorted_ohe_list.append(ohe_list[i])\n",
    "            sorted_column_names.append(column_names[i])\n",
    "\n",
    "        # Limit the number of variables to max_encoded.\n",
    "        if len(sorted_ohe_list) > max_encoded:\n",
    "            sorted_ohe_list = sorted_ohe_list[:max_encoded]\n",
    "            sorted_column_names = sorted_column_names[:max_encoded]\n",
    "        # Add the encoded data to a dataframe.\n",
    "        ohe_df = pd.DataFrame(np.column_stack(sorted_ohe_list), columns=sorted_column_names)\n",
    "        \n",
    "        if ohe_df.empty:\n",
    "            print(\"No correlations exceed the threshold.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return ohe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "1043\n",
      "47439\n",
      "0\n",
      "137\n",
      "68\n",
      "138\n",
      "65535\n",
      "65532\n",
      "1024\n",
      "60986\n",
      "65534\n",
      "65527\n",
      "80\n",
      "65529\n",
      "1029\n",
      "6881\n",
      "32820\n",
      "5190\n",
      "25\n",
      "65533\n",
      "143\n",
      "21\n",
      "65524\n",
      "53\n",
      "111\n",
      "2013\n",
      "65531\n",
      "1103\n",
      "1911\n",
      "1230\n"
     ]
    }
   ],
   "source": [
    "cte = CorrOnehotEncoder(data['sport'], data['label'])\n",
    "# dsport and sport are the categories with the highest cardinalities. Websites offer lists for suspicious ports but we don't know how relative they\n",
    "# might be to the data. So they are collected like this. I will compare the results to the ones found online to see how closely they match.\n",
    "# NOTE: Processing time has decreased by more than half since the tensor conversion and increase in sparse_n (60) in the encoder.\n",
    "# Port 0: Wildcard Port: Let the port be automatically chosen.\n",
    "# https://www.lifewire.com/port-0-in-tcp-and-udp-818145\n",
    "ec1 = cte.encode(60, 0.001, 30)\n",
    "print(len(ec1.columns))\n",
    "for i in ec1.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "53\n",
      "0\n",
      "445\n",
      "6881\n",
      "5190\n",
      "110\n",
      "22\n",
      "520\n",
      "179\n",
      "514\n",
      "5060\n",
      "143\n",
      "25\n",
      "1723\n",
      "80\n",
      "69\n",
      "137\n",
      "8080\n",
      "139\n",
      "21\n",
      "67\n",
      "135\n",
      "23\n",
      "389\n",
      "3306\n",
      "443\n",
      "5555\n",
      "161\n",
      "554\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "cte = CorrOnehotEncoder(data['dsport'], data['label'])\n",
    "ec2 = cte.encode(60, 0.001, 30)\n",
    "print(len(ec2.columns))\n",
    "for i in ec2.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "tcp\n",
      "udp\n",
      "unas\n",
      "sctp\n",
      "ospf\n",
      "any\n",
      "gre\n",
      "rsvp\n",
      "ipv6\n",
      "mobile\n",
      "sun-nd\n",
      "swipe\n",
      "pim\n",
      "sep\n",
      "arp\n",
      "etherip\n",
      "encap\n",
      "ipip\n",
      "gmtp\n",
      "sccopmce\n",
      "merit-inp\n",
      "a/n\n",
      "emcon\n",
      "nvp\n",
      "netblt\n",
      "mfe-nsp\n",
      "pri-enc\n",
      "vines\n",
      "igp\n",
      "ax.25\n"
     ]
    }
   ],
   "source": [
    "cte = CorrOnehotEncoder(data['proto'], data['label'])\n",
    "ec3 = cte.encode(60, 0.0, 30)\n",
    "print(len(ec3.columns))\n",
    "for i in ec3.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "dns\n",
      "none\n",
      "ftp-data\n",
      "pop3\n",
      "ssh\n",
      "smtp\n",
      "http\n",
      "ftp\n",
      "dhcp\n",
      "ssl\n",
      "snmp\n"
     ]
    }
   ],
   "source": [
    "cte = CorrOnehotEncoder(data['service'], data['label'])\n",
    "ec4 = cte.encode(60, 0.0, 30)\n",
    "print(len(ec4.columns))\n",
    "for i in ec4.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "INT\n",
      "FIN\n",
      "CON\n",
      "RST\n",
      "CLO\n",
      "ECO\n",
      "REQ\n"
     ]
    }
   ],
   "source": [
    "cte = CorrOnehotEncoder(data['state'], data['label'])\n",
    "ec5 = cte.encode(60, 0.0, 30)\n",
    "print(len(ec5.columns))\n",
    "for i in ec5.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "175.45.176.1\n",
      "175.45.176.3\n",
      "175.45.176.0\n",
      "175.45.176.2\n",
      "59.166.0.4\n",
      "59.166.0.1\n",
      "59.166.0.5\n",
      "59.166.0.2\n",
      "59.166.0.0\n",
      "59.166.0.3\n",
      "59.166.0.9\n",
      "59.166.0.6\n",
      "59.166.0.8\n",
      "59.166.0.7\n",
      "149.171.126.18\n",
      "149.171.126.15\n",
      "149.171.126.14\n",
      "149.171.126.10\n",
      "149.171.126.12\n",
      "10.40.85.1\n",
      "10.40.182.1\n",
      "10.40.182.6\n",
      "10.40.85.30\n",
      "10.40.182.3\n",
      "10.40.170.2\n",
      "10.40.85.10\n",
      "149.171.126.5\n",
      "149.171.126.1\n",
      "149.171.126.3\n",
      "149.171.126.4\n"
     ]
    }
   ],
   "source": [
    "# We can group IPs differently as this might be impractical since they are all coming from the same router.\n",
    "# IP addresses can be grouped by router (removing the last .value).\n",
    "# NOTE: This might be due to the nature of the data and how the USNW-NB15 dataset was created. \n",
    "# If a company is collecting historic data from previously encountered IPs they can be collected like\n",
    "# this and a model can be periodically trained for it. Alternatively the IPs could just be flagged instead of training the model on them.\n",
    "cte = CorrOnehotEncoder(data['srcip'], data['label'])\n",
    "ec6 = cte.encode(60, 0.0, 30)\n",
    "print(len(ec6.columns))\n",
    "for i in ec6.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "149.171.126.18\n",
      "149.171.126.15\n",
      "149.171.126.14\n",
      "149.171.126.10\n",
      "149.171.126.12\n",
      "149.171.126.17\n",
      "149.171.126.19\n",
      "149.171.126.13\n",
      "149.171.126.11\n",
      "149.171.126.3\n",
      "149.171.126.2\n",
      "149.171.126.4\n",
      "149.171.126.1\n",
      "149.171.126.5\n",
      "149.171.126.0\n",
      "149.171.126.9\n",
      "149.171.126.7\n",
      "149.171.126.6\n",
      "149.171.126.8\n",
      "149.171.126.16\n",
      "175.45.176.3\n",
      "175.45.176.1\n",
      "175.45.176.0\n",
      "224.0.0.5\n",
      "10.40.182.3\n",
      "10.40.182.255\n",
      "10.40.85.1\n",
      "10.40.170.2\n",
      "10.40.85.30\n",
      "59.166.0.0\n"
     ]
    }
   ],
   "source": [
    "cte = CorrOnehotEncoder(data['dstip'], data['label'])\n",
    "ec7 = cte.encode(60, 0.0, 30)\n",
    "print(len(ec7.columns))\n",
    "for i in ec7.columns:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
