{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Jupyter Notebook with SHAP and LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "import os\n",
    "import time\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import shap\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train, val, epochs, title):\n",
    "    \"\"\"\n",
    "    Plot the training, validation, and test metrics.\n",
    "\n",
    "    Parameters:\n",
    "        - train_loss (list): Training metric\n",
    "        - val_loss (list): Validation metric\n",
    "        - test_loss (list): Test tmetric\n",
    "        - epochs (int): The number of epochs.\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(range(1, epochs + 1), train, label='Training', color=sns.color_palette(\"pastel\")[0])\n",
    "    plt.plot(range(1, epochs + 1), val, label='Validation', color=sns.color_palette(\"pastel\")[1])\n",
    "    #plt.plot(range(1, epochs + 1), test, label='Test', color=sns.color_palette(\"pastel\")[2])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(f\"{title}\")\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Dataset.\n",
    "This section loads the data and creates a train, validation, and test set. Because of the large amount of Normal values, a downsampling technique was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSampler:\n",
    "    \"\"\"\n",
    "    DataSampler: Manages sampling and splitting of the USNW-NB15 dataset. Returns a training, validation, and test set.\n",
    "\n",
    "    Initialisation:\n",
    "        - train (None): The attributes that stores the train set.\n",
    "        - val (None): The attributes that stores the validation set.\n",
    "        - test (None): The attributes that stores the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.test = None\n",
    "\n",
    "    def sample_data(self, data, size, rs):\n",
    "        \"\"\"\n",
    "        sample_data: Combines the USNW-NB15 1, 2, 3, 4 sets, does some basic cleaning, Use a random number generator to take slices for the validation\n",
    "        and test set from the main data then splits into train, validation, and test set. Downsamples the data so that Normal labels match Generic in each set.\n",
    "        Uses a mask to take only Normal values that do not precede a Non-Normal value.\n",
    "\n",
    "        Parameters:\n",
    "            - data (string): The dataset to use (currently only USNW-NB15 is supported).\n",
    "            - type_of (string):  The type of sampling.\n",
    "            - rs (int): The random seed for slicing operation.\n",
    "\n",
    "        NOTE: Includes print outs to verify the process.\n",
    "        \"\"\"\n",
    "\n",
    "        if data == 'full_data':\n",
    "            feature_names = pd.read_csv('features2.csv' )\n",
    "\n",
    "            #print('Set1')\n",
    "            feature_names_list = feature_names['Name'].tolist()\n",
    "            s1 = pd.read_csv('UNSW-NB15_1.csv', header=None)\n",
    "            s1.columns = feature_names_list\n",
    "            s1.loc[s1['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "            #print(s1['attack_cat'].value_counts())\n",
    "            #print('Set2')\n",
    "            feature_names_list = feature_names['Name'].tolist()\n",
    "            s2 = pd.read_csv('UNSW-NB15_2.csv', header=None)\n",
    "            s2.columns = feature_names_list\n",
    "            s2.loc[s2['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "            #print(s2['attack_cat'].value_counts())\n",
    "            #print('Set3')\n",
    "            feature_names_list = feature_names['Name'].tolist()\n",
    "            s3 = pd.read_csv('UNSW-NB15_3.csv', header=None)\n",
    "            s3.columns = feature_names_list\n",
    "            s3.loc[s3['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "            #print(s3['attack_cat'].value_counts())\n",
    "            #print('Set4')\n",
    "            feature_names_list = feature_names['Name'].tolist()\n",
    "            s4 = pd.read_csv('UNSW-NB15_4.csv', header=None)\n",
    "            s4.columns = feature_names_list\n",
    "            s4.loc[s4['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "            #print(s4['attack_cat'].value_counts())\n",
    "            data = [s1, s2, s3, s4]\n",
    "            i = 0\n",
    "\n",
    "            while i < len(data):\n",
    "                df = data[i]\n",
    "                #print(f\"Set{i + 1}\")\n",
    "                normal = df[df['attack_cat'] == 'Normal'].shape[0]\n",
    "                generic = df[df['attack_cat'] == 'Generic'].shape[0]\n",
    "                difference = normal - generic\n",
    "                mask = df['Label'].shift(-1) != 1\n",
    "                rows = df[(df['attack_cat'] == 'Normal') & mask]\n",
    "                downsampled = rows.sample(n=difference, random_state=rs)\n",
    "                df2 = df.drop(downsampled.index)\n",
    "                #print(df2['attack_cat'].value_counts())\n",
    "                data[i] = df2\n",
    "                i += 1\n",
    "            # Clean Labels.\n",
    "            full_data = pd.concat([data[0], data[1], data[2], data[3]]).reset_index(drop=True)\n",
    "            full_data['attack_cat'] = full_data['attack_cat'].str.replace(r'\\s+', '', regex=True)\n",
    "            full_data['attack_cat'] = full_data['attack_cat'].str.replace('Backdoors', 'Backdoor')\n",
    "            # Drop Sparse Data.\n",
    "            # NOTE: These values could also be transformed by adding 1 and setting nulls to 0.\n",
    "            full_data = full_data.drop(columns=['ct_ftp_cmd', 'ct_flw_http_mthd', 'is_ftp_login'])\n",
    "            # Remove error values created by nulls.\n",
    "            full_data['sport'] = full_data['sport'].apply(pd.to_numeric)\n",
    "            full_data = full_data[~full_data['dsport'].astype(str).str.startswith('0x')]\n",
    "            full_data['dsport'] = full_data['dsport'].apply(pd.to_numeric)\n",
    "\n",
    "            slice_size = int(size * len(full_data))\n",
    "            val_start = random.randrange(0, len(full_data) - 2 * slice_size)\n",
    "            val_end = val_start + slice_size\n",
    "            val_data = full_data.iloc[val_start:val_end]\n",
    "            df = full_data.drop(val_data.index)\n",
    "            test_start = random.randrange(0, len(df) - slice_size)\n",
    "            test_end = test_start + slice_size\n",
    "            test_data = df.iloc[test_start:test_end]\n",
    "            train_data = df.drop(test_data.index)\n",
    "            #print('Train')\n",
    "            #print(len(train_data))\n",
    "            #print('Val')\n",
    "            #print(len(val_data))\n",
    "            #print('Test')\n",
    "            #print(len(test_data))\n",
    "            \n",
    "            return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocess Data\n",
    "# Handle missing values, encode categorical variables, etc.\n",
    "# Example: Fill missing values with the mean or else defined methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 For Exmaple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 For Exmaple Ramdom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Make Predictions\n",
    "# Example: Predict on new data\n",
    "# Replace 'data.csv' with the path to your new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Visualize Results\n",
    "# Example: Plot feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Model Interpretability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Model Interpretability with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
