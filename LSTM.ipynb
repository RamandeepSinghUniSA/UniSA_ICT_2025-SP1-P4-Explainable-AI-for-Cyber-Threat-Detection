{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train, val, test, epochs, title):\n",
    "    \"\"\"\n",
    "    Plot the training, validation, and test metrics.\n",
    "\n",
    "    Parameters:\n",
    "        - train_loss (list): Training metric\n",
    "        - val_loss (list): Validation metric\n",
    "        - test_loss (list): Test tmetric\n",
    "        - epochs (int): The number of epochs.\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(range(1, epochs + 1), train, label='Training', color=sns.color_palette(\"pastel\")[0])\n",
    "    plt.plot(range(1, epochs + 1), val, label='Validation', color=sns.color_palette(\"pastel\")[1])\n",
    "    plt.plot(range(1, epochs + 1), test, label='Test', color=sns.color_palette(\"pastel\")[2])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSampler:\n",
    "    \"\"\"\n",
    "    DataSampler: Manages sampling and splitting of the USNW-NB15 dataset. Returns a training, validation, and test set.\n",
    "\n",
    "    Initialisation:\n",
    "        - train (None): The attributes that stores the train set.\n",
    "        - val (None): The attributes that stores the validation set.\n",
    "        - test (None): The attributes that stores the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.test = None\n",
    "\n",
    "    def sample_data(self, data, type_of, rs):\n",
    "        \"\"\"\n",
    "        sample_data: Combines the USNW-NB15 training and test set, shuffles them, then splits into train, validation, and test set.\n",
    "\n",
    "        Parameters:\n",
    "            - data (string): The dataset to use (currently only USNW-NB15 is supported).\n",
    "            - type_of (string):  The type of sampling.\n",
    "            - rs (int): The random seed for the sampler.\n",
    "        \"\"\"\n",
    "\n",
    "        if data == 'train_test':\n",
    "            train = pd.read_csv('UNSW_NB15_training-set.csv')\n",
    "            test = pd.read_csv('UNSW_NB15_testing-set.csv')\n",
    "            data = pd.concat([train, test])\n",
    "        if type_of == 'full_random':\n",
    "            data = data.sample(frac=1, random_state=rs).reset_index(drop=True)\n",
    "            total_samples = len(data)\n",
    "            val_samples = int(total_samples * 0.15)\n",
    "            test_samples = int(total_samples * 0.15)\n",
    "            self.val = data.iloc[:val_samples]\n",
    "            self.test = data.iloc[val_samples:val_samples+test_samples]\n",
    "            self.train = data.iloc[val_samples+test_samples:]\n",
    "            return self.train, self.val, self.test\n",
    "\n",
    "        if type_of == 'full_upsample':\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"\n",
    "    DataManager: Help manage the pre-processing of the USNW-NB15 dataset for an LSTM.\n",
    "\n",
    "    Initialisation:\n",
    "        - train (pd.Dataframe): The training data.\n",
    "        - val (pd.Dataframe): The validation data.\n",
    "        - test (pd.Dataframe): The teting data.\n",
    "        - type_of (string): Specify if multiclass or other (currently only multi-class is supported).\n",
    "    \"\"\"\n",
    "    def __init__(self, train, val, test, type_of):\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.test = test\n",
    "        if type_of == 'multi':\n",
    "            for i in [self.train, self.val, self.test]:\n",
    "                i.drop(columns=['id', 'label'], inplace=True)    \n",
    "            self.label = 'attack_cat'\n",
    "        \n",
    "    def label_encode(self):\n",
    "        \"\"\"\n",
    "        label_encode: label encodes the categorical columns. Preset to 'proto', 'state', and 'service' and the label.\n",
    "\n",
    "        NOTE: This can have conflicts depending on what seed is used in the sampler.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data = [self.train, self.val, self.test]\n",
    "        columns = ['proto', 'state', 'service', self.label]\n",
    "        encoder = LabelEncoder()\n",
    "        for col in columns:\n",
    "            for df in data:\n",
    "                df[col] = encoder.fit_transform(df[col])\n",
    "        self.train, self.val, self.test = data\n",
    "    \n",
    "    def onehot_encode(self):\n",
    "        \"\"\"\n",
    "        onehot_encode: One-hot encodes the categorical columns. Preset to 'proto', 'state', and 'service'. Label encodes the target.\n",
    "\n",
    "        NOTE: This can have conflicts depending on what seed is used in the sampler.\n",
    "        \"\"\"\n",
    "        columns = ['proto', 'state', 'service']\n",
    "\n",
    "        encoder2 = LabelEncoder()\n",
    "        \n",
    "        encoder = OneHotEncoder(sparse_output=False, dtype='float32')\n",
    "\n",
    "        for col in columns:\n",
    "            encoded_train = encoder.fit_transform(self.train[[col]])\n",
    "            encoded_val = encoder.transform(self.val[[col]])\n",
    "            encoded_test = encoder.transform(self.test[[col]])\n",
    "            encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out([col]), index=self.train.index)\n",
    "            encoded_val_df = pd.DataFrame(encoded_val, columns=encoder.get_feature_names_out([col]), index=self.val.index)\n",
    "            encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out([col]), index=self.test.index)\n",
    "            self.train = pd.concat([self.train.drop(columns=[col]), encoded_train_df], axis=1)\n",
    "            self.val = pd.concat([self.val.drop(columns=[col]), encoded_val_df], axis=1)\n",
    "            self.test = pd.concat([self.test.drop(columns=[col]), encoded_test_df], axis=1)\n",
    "\n",
    "        encoder = LabelEncoder()\n",
    "        for df in [self.train, self.val, self.test]:\n",
    "            df[self.label] = encoder.fit_transform(df[self.label])\n",
    "\n",
    "\n",
    "    def standardise(self, type_of):\n",
    "        \"\"\"\n",
    "        standardise: Standardises the features of the train and test datasets.\n",
    "\n",
    "        Parameters:\n",
    "            - type_of (string): Choose the standardisation ('standard' or 'minmax').\n",
    "        \"\"\"\n",
    "        train_features = self.train.drop(columns=[self.label])\n",
    "        val_features = self.val.drop(columns=[self.label])\n",
    "        test_features = self.test.drop(columns=[self.label])\n",
    "        train_label = self.train[self.label]\n",
    "        val_label = self.val[self.label]\n",
    "        test_label = self.test[self.label]\n",
    "        if type_of == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif type_of == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        scaled_train = scaler.fit_transform(train_features)\n",
    "        scaled_val = scaler.transform(val_features)\n",
    "        scaled_test = scaler.transform(test_features)\n",
    "        self.train = pd.DataFrame(scaled_train, columns=train_features.columns, index=self.train.index)\n",
    "        self.train[self.label] = train_label\n",
    "        self.val = pd.DataFrame(scaled_val, columns=val_features.columns, index=self.val.index)\n",
    "        self.val[self.label] = val_label\n",
    "        self.test = pd.DataFrame(scaled_test, columns=test_features.columns, index=self.test.index)\n",
    "        self.test[self.label] = test_label\n",
    "\n",
    "    def get_sequence(self, batch_size):\n",
    "        \"\"\"\n",
    "        get_sequence: Converts the Dtaframe to a sequence for each row and the corresponding label.\n",
    "        \n",
    "        Parameters:\n",
    "            - batch_size (int): The batch size to use in DataLoader.\n",
    "\n",
    "        Returns:\n",
    "            - train_loader (DataLoader): Dataloader for the training data.\n",
    "            - val_loader (DataLoader): Dataloader for the validation data.\n",
    "            - test_loader (DataLoader): Dataloader for the test data.\n",
    "        \"\"\"\n",
    "        \n",
    "        train_seq = torch.tensor(self.train.drop(columns=[self.label]).values, dtype=torch.float32)\n",
    "        val_seq = torch.tensor(self.val.drop(columns=[self.label]).values, dtype=torch.float32)\n",
    "        test_seq = torch.tensor(self.test.drop(columns=[self.label]).values, dtype=torch.float32)\n",
    "        train_label = torch.tensor(self.train[self.label].values, dtype=torch.long)\n",
    "        val_label = torch.tensor(self.val[self.label].values, dtype=torch.long)\n",
    "        test_label = torch.tensor(self.test[self.label].values, dtype=torch.long)\n",
    "\n",
    "        # Add dimension for LSTM.\n",
    "        train_seq = train_seq.unsqueeze(1)\n",
    "        val_seq = val_seq.unsqueeze(1)\n",
    "        test_seq = test_seq.unsqueeze(1)\n",
    "\n",
    "        train_dataset = TensorDataset(train_seq, train_label)\n",
    "        val_dataset = TensorDataset(val_seq, val_label)\n",
    "        test_dataset = TensorDataset(test_seq, test_label)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    SimpleLSTM: An LSTM Model customised for the USNW-NB15 dataset. Uses a fully connected layer to output the predictions.\n",
    "\n",
    "    Parameters:\n",
    "        - nn.Module (nn.Module): The torch Neural Network module.\n",
    "    \n",
    "    Initialisation:\n",
    "        - n_features (int): Number of features:\n",
    "        - hidden_size (int): Hidden layer size.\n",
    "        - n_layers (int): Number of hidden layers.\n",
    "        - n_labels (int): Number of labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, hidden_size, n_layers, n_labels, learning_rate):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        # LSTM Layer (doubled).\n",
    "        self.lstm = nn.LSTM(n_features, \n",
    "                            hidden_size, \n",
    "                            n_layers, \n",
    "                            batch_first=True)\n",
    "        # Fully connected layer to the number of labels.\n",
    "        self.fc = nn.Linear(hidden_size, n_labels)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = None\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.save_dir = 'saved_model/lstm/'\n",
    "        # Metrics to track\n",
    "        self.loss = []\n",
    "        self.accuracy = []\n",
    "        self.f1 = []\n",
    "        self.epoch_time = []\n",
    "        self.val_loss = []\n",
    "        self.val_accuracy = []\n",
    "        self.test_loss = []\n",
    "        self.test_accuracy = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward: The forward function for calls the lstm layer and collects the last hidden state. \n",
    "        Finally uses a fully connected layer to output the logits for softmax.\n",
    "\n",
    "        Parameters:\n",
    "            - x (array): The batch array from the Dataloader.\n",
    "\n",
    "        Returns:\n",
    "            - out (array): The logits (10) of the fully connected layer.\n",
    "\n",
    "        NOTE: We are using row-by to same label sequence.\n",
    "        \"\"\"\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        # The last hidden state.\n",
    "        final_hidden_state = hn[-1]\n",
    "        # Fully connected to label count.\n",
    "        out = self.fc(final_hidden_state)\n",
    "        return out\n",
    "\n",
    "    def run(self, train_loader, val_loader, test_loader, epochs, save_factor):\n",
    "        \"\"\"\n",
    "        run: Go through each epoch and do a forward pass then backpropagate using Cross Entropy Loss and Adam Optimiser to adjust the gradient. \n",
    "        Then evaluate the epoch using a validation set. At the end of all epochs, evaluate using the test set.\n",
    "\n",
    "        Parameters:\n",
    "            - train_loader (Dataloader): The Dataloader with the training set and labels.\n",
    "            - val_loader (Dataloader): The Dataloader with the validation set and labels.\n",
    "            - test_loader (Dataloader): The Dataloader with the test set and labels.\n",
    "            - epochs (int): The number of epochs to train.\n",
    "            - save_factor (int): The factor at which epoch to save the model weights.\n",
    "        \"\"\"\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            # Set to training mode.\n",
    "            self.train()\n",
    "            # Reset metrics for epoch.\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{epochs}\") as tqmbar:\n",
    "                for batch_idx, (inputs, labels) in enumerate(tqmbar):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    # Reset gradient.\n",
    "                    self.optimizer.zero_grad()\n",
    "                    # Forward.\n",
    "                    outputs = self(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    # Backpropagation.\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    # Cumulative loss.\n",
    "                    running_loss += loss.item()\n",
    "                    # Get highest probability label.\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            accuracy = 100 * correct / total\n",
    "            if len(all_preds) > 0:\n",
    "                f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "            else:\n",
    "                f1 = 0\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.2f}\")\n",
    "            self.loss.append(avg_loss)\n",
    "            self.accuracy.append(accuracy)\n",
    "            self.f1.append(f1)\n",
    "            self.epoch_time.append(time.time() - start_time)\n",
    "\n",
    "            # Validation after each epoch\n",
    "            val_loss, val_accuracy, val_f1 = self.validate(val_loader)\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%, Validation F1 Score: {val_f1:.2f}\")\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_accuracy.append(val_accuracy)\n",
    "\n",
    "            # Test after each epoch\n",
    "            test_loss, test_accuracy, test_f1 = self.test(test_loader)\n",
    "            print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Test F1 Score: {test_f1:.2f}\")\n",
    "            self.test_loss.append(test_loss)\n",
    "            self.test_accuracy.append(test_accuracy)\n",
    "\n",
    "            # Save model at specified intervals.\n",
    "            if (epoch + 1) % save_factor == 0:\n",
    "                self.save_model(epoch + 1)\n",
    "\n",
    "    def validate(self, data_loader):\n",
    "        \"\"\"\n",
    "        validate: The validation run of the LSTM model.\n",
    "\n",
    "        Parameters:\n",
    "            - data_loader (Dataloader): The Dataloader of validation set.\n",
    "        \n",
    "        Returns:\n",
    "            - loss (float): The average loss of the validation run.\n",
    "            - accuracy (float): The accuracy of validation run.\n",
    "            - f1 (float): The F1 score (macro) of validation run.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = self(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        avg_loss = running_loss / len(data_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        return avg_loss, accuracy, f1\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        \"\"\"\n",
    "        test: The test run of the LSTM model.\n",
    "\n",
    "        Parameters:\n",
    "            - data_loader (Dataloader): The Dataloader of test set.\n",
    "        \n",
    "        Returns:\n",
    "            - loss (float): The average loss of the test run.\n",
    "            - accuracy (float): The accuracy of test run.\n",
    "            - f1 (float): The F1 score (macro) of test run.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = self(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = running_loss / len(test_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        return avg_loss, accuracy, f1\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        \"\"\"\n",
    "        predict: Predicts the class for the input.\n",
    "\n",
    "        Parameters:\n",
    "            - input_data (tensor): The input data.\n",
    "        \n",
    "        Returns:\n",
    "            - predicted (tensor): The predicted labels.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        input_data = input_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = self(input_data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "        return predicted\n",
    "\n",
    "    def save_model(self, epoch):\n",
    "        \"\"\"\n",
    "        save_model: Save the model's state and metrics to directory for the given epoch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The current epoch.\n",
    "        \"\"\"\n",
    "        checkpoint_path = os.path.join(self.save_dir, f\"lstm_epoch_{epoch}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': self.criterion,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Model saved to {checkpoint_path}\")\n",
    "\n",
    "    def load_model(self, epoch):\n",
    "        \"\"\"\n",
    "        load_model: Load a saved model from the directory.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch to load.\n",
    "    \n",
    "        \"\"\"\n",
    "        checkpoint_path = os.path.join(self.save_dir, f\"lstm_epoch_{epoch}.pt\")\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            self.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  33%|███▎      | 1836/5637 [00:05<00:09, 394.42batch/s]"
     ]
    }
   ],
   "source": [
    "# Sample Full Random Datasets.\n",
    "sampler = DataSampler()\n",
    "train, val, test = sampler.sample_data('train_test', 'full_random', 42)\n",
    "dm = DataManager(train, val, test, 'multi')\n",
    "batch_size = 32\n",
    "dm.label_encode()\n",
    "dm.standardise('standard')\n",
    "train_loader, val_loader, test_loader = dm.get_sequence(batch_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_features = 42\n",
    "hidden_size = 128\n",
    "n_layers = 2\n",
    "n_labels = 10\n",
    "\n",
    "model = SimpleLSTM(n_features, hidden_size, n_layers, n_labels, 0.0001).to(device)\n",
    "# Datasets, Epochs, and Save Factor.\n",
    "model.run(train_loader, val_loader, test_loader, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(model.train_loss, model.val_loss, model.test_loss, 10, 'LSTM Loss')\n",
    "plot_metrics(model.train_accuracy, model.val_accuracy, model.test_accuracy, 10, 'LSTM Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda311new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
