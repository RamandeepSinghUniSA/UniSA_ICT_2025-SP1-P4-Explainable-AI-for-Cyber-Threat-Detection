{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import time\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import shap\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from captum.attr import IntegratedGradients\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train, val, epochs, title):\n",
    "    \"\"\"\n",
    "    Plot the training, validation, and test metrics.\n",
    "\n",
    "    Parameters:\n",
    "        - train_loss (list): Training metric\n",
    "        - val_loss (list): Validation metric\n",
    "        - test_loss (list): Test tmetric\n",
    "        - epochs (int): The number of epochs.\n",
    "    \"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(range(1, epochs + 1), train, label='Training', color=sns.color_palette(\"pastel\")[0])\n",
    "    plt.plot(range(1, epochs + 1), val, label='Validation', color=sns.color_palette(\"pastel\")[1])\n",
    "    #plt.plot(range(1, epochs + 1), test, label='Test', color=sns.color_palette(\"pastel\")[2])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(f\"{title}\")\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSampler:\n",
    "    \"\"\"\n",
    "    DataSampler: Manages sampling and splitting of the USNW-NB15 dataset. Returns a training, validation, and test set.\n",
    "\n",
    "    Initialisation:\n",
    "        - train (None): The attributes that stores the train set.\n",
    "        - val (None): The attributes that stores the validation set.\n",
    "        - test (None): The attributes that stores the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.test = None\n",
    "\n",
    "    def sample_data(self, data, size, rs, target_label):\n",
    "        \"\"\"\n",
    "        sample_data: Combines the USNW-NB15 1, 2, 3, 4 sets, does some basic cleaning, Use a random number generator to take slices for the validation\n",
    "        and test set from the main data then splits into train, validation, and test set. Downsamples the data so that Normal labels match Generic in each set.\n",
    "        Uses a mask to take only Normal values that do not precede a Non-Normal value.\n",
    "\n",
    "        Parameters:\n",
    "            - data (string): The dataset to use (currently only USNW-NB15 is supported).\n",
    "            - type_of (string):  The type of sampling.\n",
    "            - rs (int): The random seed for slicing operation.\n",
    "\n",
    "        NOTE: Includes print outs to verify the process.\n",
    "        \"\"\"\n",
    "\n",
    "        if data == 'full_data':\n",
    "            feature_names = pd.read_csv('features2.csv' )\n",
    "\n",
    "            #print('Set1')\n",
    "            feature_names_list = feature_names['Name'].tolist()\n",
    "            s1 = pd.read_csv('UNSW-NB15_1.csv', header=None)\n",
    "            s1.columns = feature_names_list\n",
    "            s1.loc[s1['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "            #print(s1['attack_cat'].value_counts())\n",
    "            #print('Set2')\n",
    "            feature_names_list = feature_names['Name'].tolist()\n",
    "            s2 = pd.read_csv('UNSW-NB15_2.csv', header=None)\n",
    "            s2.columns = feature_names_list\n",
    "            s2.loc[s2['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "            #print(s2['attack_cat'].value_counts())\n",
    "            #print('Set3')\n",
    "            feature_names_list = feature_names['Name'].tolist()\n",
    "            s3 = pd.read_csv('UNSW-NB15_3.csv', header=None)\n",
    "            s3.columns = feature_names_list\n",
    "            s3.loc[s3['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "            #print(s3['attack_cat'].value_counts())\n",
    "            #print('Set4')\n",
    "            feature_names_list = feature_names['Name'].tolist()\n",
    "            s4 = pd.read_csv('UNSW-NB15_4.csv', header=None)\n",
    "            s4.columns = feature_names_list\n",
    "            s4.loc[s4['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "            #print(s4['attack_cat'].value_counts())\n",
    "            data = [s1, s2, s3, s4]\n",
    "            i = 0\n",
    "\n",
    "            while i < len(data):\n",
    "                df = data[i]\n",
    "                #print(f\"Set{i + 1}\")\n",
    "                normal = df[df['attack_cat'] == 'Normal'].shape[0]\n",
    "                generic = df[df['attack_cat'] == 'Generic'].shape[0]\n",
    "                difference = normal - generic\n",
    "                mask = df['Label'].shift(-1) != 1\n",
    "                rows = df[(df['attack_cat'] == 'Normal') & mask]\n",
    "                downsampled = rows.sample(n=difference, random_state=rs)\n",
    "                df2 = df.drop(downsampled.index)\n",
    "                #print(df2['attack_cat'].value_counts())\n",
    "                data[i] = df2\n",
    "                i += 1\n",
    "            # Clean Labels.\n",
    "            full_data = pd.concat([data[0], data[1], data[2], data[3]]).reset_index(drop=True)\n",
    "            full_data['attack_cat'] = full_data['attack_cat'].str.replace(r'\\s+', '', regex=True)\n",
    "            full_data['attack_cat'] = full_data['attack_cat'].str.replace('Backdoors', 'Backdoor')\n",
    "            # Drop Sparse Data.\n",
    "            # NOTE: These values could also be transformed by adding 1 and setting nulls to 0.\n",
    "            full_data = full_data.drop(columns=['ct_ftp_cmd', 'ct_flw_http_mthd', 'is_ftp_login'])\n",
    "            # Remove error values created by nulls.\n",
    "            full_data['sport'] = full_data['sport'].apply(pd.to_numeric)\n",
    "            full_data = full_data[~full_data['dsport'].astype(str).str.startswith('0x')]\n",
    "            full_data['dsport'] = full_data['dsport'].apply(pd.to_numeric)\n",
    "            full_data['attack_cat'] = (full_data['attack_cat'] == target_label).astype(int)\n",
    "            slice_size = int(size * len(full_data))\n",
    "            val_start = random.randrange(0, len(full_data) - 2 * slice_size)\n",
    "            val_end = val_start + slice_size\n",
    "            val_data = full_data.iloc[val_start:val_end]\n",
    "            df = full_data.drop(val_data.index)\n",
    "            test_start = random.randrange(0, len(df) - slice_size)\n",
    "            test_end = test_start + slice_size\n",
    "            test_data = df.iloc[test_start:test_end]\n",
    "            train_data = df.drop(test_data.index)\n",
    "\n",
    "            #print('Train')\n",
    "            #print(len(train_data))\n",
    "            #print('Val')\n",
    "            #print(len(val_data))\n",
    "            #print('Test')\n",
    "            #print(len(test_data))\n",
    "            \n",
    "            return train_data, val_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"\n",
    "    DataManager: Help manage the pre-processing of the USNW-NB15 dataset for an LSTM.\n",
    "\n",
    "    Initialisation:\n",
    "        - train (pd.Dataframe): The training data.\n",
    "        - val (pd.Dataframe): The validation data.\n",
    "        - test (pd.Dataframe): The teting data.\n",
    "        - type_of (string): Specify if multiclass or other (currently only multi-class is supported).\n",
    "    \"\"\"\n",
    "    def __init__(self, train, val, test, type_of):\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.test = test\n",
    "        if type_of == 'multi':\n",
    "            for i in [self.train, self.val, self.test]:\n",
    "                i.drop(columns=['Label'], inplace=True)    \n",
    "            self.label = 'attack_cat'\n",
    "        if type_of == 'binary':\n",
    "            for i in [self.train, self.val, self.test]:\n",
    "                i.drop(columns=['attack_cat'], inplace=True)    \n",
    "            self.label = 'Label'\n",
    "        \n",
    "    def label_encode(self):\n",
    "        \"\"\"\n",
    "        label_encode: label encodes the categorical columns. Preset to 'proto', 'state', and 'service' and the label.\n",
    "\n",
    "        NOTE: This can have conflicts depending on what seed is used in the sampler.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data = [self.train, self.val, self.test]\n",
    "        columns = ['srcip', 'dstip', 'proto', 'state', 'service', self.label]\n",
    "        encoder = LabelEncoder()\n",
    "        for col in columns:\n",
    "            for df in data:\n",
    "                df[col] = encoder.fit_transform(df[col])\n",
    "        self.train, self.val, self.test = data\n",
    "    \n",
    "    def onehot_encode(self):\n",
    "        \"\"\"\n",
    "        onehot_encode: One-hot encodes the categorical columns. Preset to 'proto', 'state', and 'service'. Label encodes the target.\n",
    "\n",
    "        NOTE: This can have conflicts depending on what seed is used in the sampler.\n",
    "        \"\"\"\n",
    "        columns = ['srcip', 'dstip', 'proto', 'state', 'service']\n",
    "\n",
    "        encoder2 = LabelEncoder()\n",
    "        encoder = OneHotEncoder(sparse_output=False, dtype='float32')\n",
    "\n",
    "        for col in columns:\n",
    "            encoded_train = encoder.fit_transform(self.train[[col]])\n",
    "            encoded_val = encoder.transform(self.val[[col]])\n",
    "            encoded_test = encoder.transform(self.test[[col]])\n",
    "            encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out([col]), index=self.train.index)\n",
    "            encoded_val_df = pd.DataFrame(encoded_val, columns=encoder.get_feature_names_out([col]), index=self.val.index)\n",
    "            encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out([col]), index=self.test.index)\n",
    "            self.train = pd.concat([self.train.drop(columns=[col]), encoded_train_df], axis=1)\n",
    "            self.val = pd.concat([self.val.drop(columns=[col]), encoded_val_df], axis=1)\n",
    "            self.test = pd.concat([self.test.drop(columns=[col]), encoded_test_df], axis=1)\n",
    "\n",
    "        encoder = LabelEncoder()\n",
    "        for df in [self.train, self.val, self.test]:\n",
    "            df[self.label] = encoder.fit_transform(df[self.label])\n",
    "\n",
    "\n",
    "    def standardise(self, type_of):\n",
    "        \"\"\"\n",
    "        standardise: Standardises the features of the train and test datasets.\n",
    "\n",
    "        Parameters:\n",
    "            - type_of (string): Choose the standardisation ('standard' or 'minmax').\n",
    "        \"\"\"\n",
    "        train_features = self.train.drop(columns=[self.label])\n",
    "        val_features = self.val.drop(columns=[self.label])\n",
    "        test_features = self.test.drop(columns=[self.label])\n",
    "        train_label = self.train[self.label]\n",
    "        val_label = self.val[self.label]\n",
    "        test_label = self.test[self.label]\n",
    "        if type_of == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif type_of == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        scaled_train = scaler.fit_transform(train_features)\n",
    "        scaled_val = scaler.transform(val_features)\n",
    "        scaled_test = scaler.transform(test_features)\n",
    "        self.train = pd.DataFrame(scaled_train, columns=train_features.columns, index=self.train.index)\n",
    "        self.train[self.label] = train_label\n",
    "        self.val = pd.DataFrame(scaled_val, columns=val_features.columns, index=self.val.index)\n",
    "        self.val[self.label] = val_label\n",
    "        self.test = pd.DataFrame(scaled_test, columns=test_features.columns, index=self.test.index)\n",
    "        self.test[self.label] = test_label\n",
    "\n",
    "    def get_sequence(self):\n",
    "        \"\"\"\n",
    "        get_sequence: Converts the Dtaframe to a sequence for each row and the corresponding label.\n",
    "        \n",
    "        Parameters:\n",
    "            - batch_size (int): The batch size to use in DataLoader.\n",
    "\n",
    "        Returns:\n",
    "            - train_loader (DataLoader): Dataloader for the training data.\n",
    "            - val_loader (DataLoader): Dataloader for the validation data.\n",
    "            - test_loader (DataLoader): Dataloader for the test data.\n",
    "        \"\"\"\n",
    "        self.test = self.test.sample(n=1000, random_state=42)\n",
    "        train_seq = torch.tensor(self.train.drop(columns=[self.label]).values, dtype=torch.float32)\n",
    "        val_seq = torch.tensor(self.val.drop(columns=[self.label]).values, dtype=torch.float32)\n",
    "        test_seq = torch.tensor(self.test.drop(columns=[self.label]).values, dtype=torch.float32)\n",
    "        train_label = torch.tensor(self.train[self.label].values, dtype=torch.float32)\n",
    "        val_label = torch.tensor(self.val[self.label].values, dtype=torch.float32)\n",
    "        test_label = torch.tensor(self.test[self.label].values, dtype=torch.float32)\n",
    "\n",
    "        # Add dimension for LSTM.\n",
    "        train_seq = train_seq.unsqueeze(1)\n",
    "        val_seq = val_seq.unsqueeze(1)\n",
    "        test_seq = test_seq.unsqueeze(1)\n",
    "\n",
    "        train_dataset = TensorDataset(train_seq, train_label)\n",
    "        val_dataset = TensorDataset(val_seq, val_label)\n",
    "        test_dataset = TensorDataset(test_seq, test_label)\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    def get_weights(self, type_of):\n",
    "        \"\"\"\n",
    "        get_weights: Calculate the weights of the labels for the given dataset. Used in Cross Entropy Loss criterion.\n",
    "        \n",
    "        Parameters:\n",
    "            - type_of (string): The dataset to use (train, val, test).\n",
    "\n",
    "        Returns:\n",
    "            - class_weights (tensor): The class weights for the label.\n",
    "        \"\"\"\n",
    "        if type_of == 'train':\n",
    "            data = self.train\n",
    "        elif type_of == 'val':\n",
    "            data = self.val\n",
    "        elif type_of == 'test':\n",
    "            data = self.test\n",
    "        labels = data[self.label].values\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "        return torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, device, save_dir):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.device = device\n",
    "        self.save_dir = save_dir\n",
    "        self.to(self.device)\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.train_f1 = []\n",
    "        self.val_f1 = []\n",
    "        self.epoch_time = []\n",
    "        self.test_accuracy = None\n",
    "        self.test_loss = None\n",
    "        self.test_f1 = None\n",
    "        self.test_predicted = []\n",
    "        self.test_actual = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward: The forward pass for the LSTM. Calculates the label for the same row.\n",
    "        This is a magic method called when the model is called (model(input)).\n",
    "\n",
    "        Parameters:\n",
    "            - x (tensor): The input sequence for the LSTM.\n",
    "\n",
    "        Returns:\n",
    "            - output (tensor): Returns the output after the LSTM layer.\n",
    "        \"\"\"\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        final_hidden_state = lstm_out[:, -1, :]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "    def calculate_accuracy_f1(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        calculate_accuracy_f1: A helper function to help modularize the code. Calculates accuracy and\n",
    "        f1 macro (average).\n",
    "\n",
    "        Parameters:\n",
    "            - predictions (tensor): The predictions given by BCEWithLogitsLoss()\n",
    "            - labels (tensor): The actual labels of the test data.\n",
    "\n",
    "        Returns:\n",
    "            - accuracy (float): The accuracy score.\n",
    "            - f1 (float): The macro f1 score.\n",
    "        \"\"\"\n",
    "        preds = torch.sigmoid(predictions)\n",
    "        preds = (preds > 0.5).float()\n",
    "        accuracy = accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1 = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        return accuracy, f1\n",
    "\n",
    "    def run(self, train_data, val_data, test_data, criterion, optimizer, epochs, save_factor):\n",
    "        \"\"\"\n",
    "        run: Run the model on the train, val data for each epoch. Evaluate on the test data at the end\n",
    "        of all epochs. Returns an explainer object based on the gradient of all datapoints in the test set and model.\n",
    "\n",
    "        Parameters:\n",
    "            - train_data (TensorDataset): The tensor holding the train features and labels.\n",
    "            - val_data (TensorDataset): The tensor holding the validation features and labels.\n",
    "            - test_data (TensorDataset): The tensor holding the test features and labels.\n",
    "            - criterion (nn.BCEWithLogitsLoss): The probability calculation for logits.\n",
    "            - optimizer (optim.Adam): The automatic regularization optimizer.\n",
    "            - epochs (int): The number of epochs to train.\n",
    "            - save_factor (int): The factor at which to save the model during training.\n",
    "\n",
    "        Returns:\n",
    "            - explainer (shap.GradientExplainer): The calculated explainer for all data on the\n",
    "            test set.\n",
    "        \"\"\"\n",
    "        # Split the tensors.\n",
    "        train_seq, train_label = train_data.tensors\n",
    "        val_seq, val_label = val_data.tensors\n",
    "        test_seq, test_label = test_data.tensors\n",
    "        train_seq, train_label = train_seq.to(self.device), train_label.to(self.device)\n",
    "        val_seq, val_label = val_seq.to(self.device), val_label.to(self.device)\n",
    "        test_seq, test_label = test_seq.to(self.device), test_label.to(self.device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass.\n",
    "            outputs = self(train_seq)\n",
    "            loss = criterion(outputs.squeeze(), train_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_accuracy, train_f1 = self.calculate_accuracy_f1(outputs, train_label)\n",
    "\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = self(val_seq)\n",
    "                val_loss = criterion(val_outputs.squeeze(), val_label)\n",
    "                val_accuracy, val_f1 = self.calculate_accuracy_f1(val_outputs, val_label)\n",
    "            self.train_loss.append(loss.item())\n",
    "            self.val_loss.append(val_loss.item())\n",
    "            self.train_accuracy.append(train_accuracy)\n",
    "            self.val_accuracy.append(val_accuracy)\n",
    "            self.train_f1.append(train_f1)\n",
    "            self.val_f1.append(val_f1)\n",
    "            epoch_time = time.time() - start_time\n",
    "            self.epoch_time.append(epoch_time)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "            if (epoch + 1) % save_factor == 0:\n",
    "                self.save_model(epoch + 1)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = self(test_seq)\n",
    "            test_loss = criterion(test_outputs.squeeze(), test_label)\n",
    "            test_accuracy, test_f1 = self.calculate_accuracy_f1(test_outputs, test_label)\n",
    "            self.test_accuracy = test_accuracy\n",
    "            self.test_loss = test_loss.item()\n",
    "            self.test_f1 = test_f1\n",
    "\n",
    "            self.test_pred = (torch.sigmoid(test_outputs) > 0.5).cpu().numpy()\n",
    "            self.test_actual = test_label.cpu().numpy()\n",
    "            print(f\"Test Loss: {self.test_loss:.4f}, Test Accuracy: {self.test_accuracy:.4f}, Test F1: {self.test_f1:.4f}\")\n",
    "\n",
    "        # NOTE: We cannot use no grad with SHAP explainers due to conflicts.\n",
    "        explainer = shap.GradientExplainer(self, test_seq)\n",
    "        return explainer\n",
    "\n",
    "    def save_model(self, epoch):\n",
    "        \"\"\"\n",
    "        save_model: Save the model state_dict for the given factor. Approx 1 megabyte each.\n",
    "\n",
    "        Parameters:\n",
    "            - The current epoch that is being saved.\n",
    "        \"\"\"\n",
    "        checkpoint_path = os.path.join(self.save_dir, f\"lstm_epoch_{epoch}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    def load_model(self, epoch):\n",
    "        \"\"\"\n",
    "        load_model: Load a state_dict of a model.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch to load.\n",
    "        \"\"\"\n",
    "        checkpoint_path = os.path.join(self.save_dir, f\"lstm_epoch_{epoch}.pt\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryEnsembler:\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "        self.data_ensemble = []\n",
    "        self.model_ensemble = []\n",
    "        self.explainers_ensemble = []\n",
    "        self.tree_ensemble = []\n",
    "        for label in self.labels:\n",
    "            df = DataSampler()\n",
    "            train_data, val_data, test_data = df.sample_data('full_data', 0.2, 42, label)\n",
    "            dm = DataManager(train_data, val_data, test_data, 'multi')\n",
    "            dm.label_encode()\n",
    "            dm.standardise('minmax')\n",
    "            train_data, val_data, test_data = dm.get_sequence()\n",
    "            device = torch.device(\"cpu\")\n",
    "            model = LSTM_Model(44, 128, 1, device, 'model_ensemble')\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "            explainer = model.run(train_data, val_data, test_data, criterion, optimizer, 10, 50)\n",
    "            self.model_ensemble.append(model)\n",
    "            self.explainers_ensemble.append(explainer)\n",
    "            tr_data, labels = train_data.tensors\n",
    "            tr_data = tr_data.squeeze()\n",
    "            tr_data = pd.DataFrame(tr_data)\n",
    "            tr_data['labels'] = pd.DataFrame(labels)\n",
    "            te_data, labels = test_data.tensors\n",
    "            te_data = te_data.squeeze()\n",
    "            te_data = pd.DataFrame(te_data)\n",
    "            te_data['labels'] = pd.DataFrame(labels)\n",
    "            X_train = tr_data.drop(columns='labels')\n",
    "            y_train = tr_data['labels']\n",
    "            X_test = te_data.drop(columns='labels')\n",
    "            y_test = te_data['labels']\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f'Accuracy: {accuracy:.4f}')\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            self.tree_ensemble.append(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_17404\\3378984335.py:35: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  s1 = pd.read_csv('UNSW-NB15_1.csv', header=None)\n",
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_17404\\3378984335.py:41: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  s2 = pd.read_csv('UNSW-NB15_2.csv', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: -0.0000, Train Accuracy: 0.0000, Train F1: 0.0000\n",
      "Val Loss: -0.0000, Val Accuracy: 0.0000, Val F1: 0.0000\n",
      "Epoch [2/10], Train Loss: -0.0000, Train Accuracy: 0.0000, Train F1: 0.0000\n",
      "Val Loss: -0.0000, Val Accuracy: 0.0000, Val F1: 0.0000\n",
      "Epoch [3/10], Train Loss: -0.0000, Train Accuracy: 0.0000, Train F1: 0.0000\n",
      "Val Loss: -0.0000, Val Accuracy: 0.0000, Val F1: 0.0000\n",
      "Epoch [4/10], Train Loss: -0.0000, Train Accuracy: 0.0000, Train F1: 0.0000\n",
      "Val Loss: -0.0000, Val Accuracy: 0.0000, Val F1: 0.0000\n",
      "Epoch [5/10], Train Loss: -0.0000, Train Accuracy: 0.0000, Train F1: 0.0000\n",
      "Val Loss: -0.0000, Val Accuracy: 0.0000, Val F1: 0.0000\n",
      "Epoch [6/10], Train Loss: -0.0000, Train Accuracy: 0.0000, Train F1: 0.0000\n",
      "Val Loss: -0.0000, Val Accuracy: 0.0000, Val F1: 0.0000\n",
      "Epoch [7/10], Train Loss: -0.0000, Train Accuracy: 0.0000, Train F1: 0.0000\n",
      "Val Loss: -0.0000, Val Accuracy: 0.0000, Val F1: 0.0000\n",
      "Epoch [8/10], Train Loss: -0.0000, Train Accuracy: 0.0000, Train F1: 0.0000\n",
      "Val Loss: -0.0000, Val Accuracy: 0.0000, Val F1: 0.0000\n",
      "Epoch [9/10], Train Loss: -0.0000, Train Accuracy: 0.0000, Train F1: 0.0000\n",
      "Val Loss: -0.0000, Val Accuracy: 0.0000, Val F1: 0.0000\n",
      "Epoch [10/10], Train Loss: -0.0000, Train Accuracy: 0.0000, Train F1: 0.0000\n",
      "Val Loss: -0.0000, Val Accuracy: 0.0000, Val F1: 0.0000\n",
      "Test Loss: -0.0000, Test Accuracy: 0.0000, Test F1: 0.0000\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_17404\\3378984335.py:35: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  s1 = pd.read_csv('UNSW-NB15_1.csv', header=None)\n",
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_17404\\3378984335.py:41: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  s2 = pd.read_csv('UNSW-NB15_2.csv', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 201484.8906, Train Accuracy: 0.0493, Train F1: 0.0046\n",
      "Val Loss: 42903.0742, Val Accuracy: 0.4401, Val F1: 0.5748\n",
      "Epoch [2/10], Train Loss: 200386.3750, Train Accuracy: 0.4572, Train F1: 0.5786\n",
      "Val Loss: 42604.7734, Val Accuracy: 0.6842, Val F1: 0.7830\n",
      "Epoch [3/10], Train Loss: 199169.8906, Train Accuracy: 0.6798, Train F1: 0.7701\n",
      "Val Loss: 42227.5078, Val Accuracy: 0.7975, Val F1: 0.8603\n",
      "Epoch [4/10], Train Loss: 197643.8125, Train Accuracy: 0.7479, Train F1: 0.8188\n",
      "Val Loss: 41754.2500, Val Accuracy: 0.8255, Val F1: 0.8780\n",
      "Epoch [5/10], Train Loss: 195754.3750, Train Accuracy: 0.7719, Train F1: 0.8352\n",
      "Val Loss: 41186.5039, Val Accuracy: 0.8404, Val F1: 0.8873\n",
      "Epoch [6/10], Train Loss: 193533.9531, Train Accuracy: 0.7842, Train F1: 0.8434\n",
      "Val Loss: 40544.1406, Val Accuracy: 0.8502, Val F1: 0.8934\n",
      "Epoch [7/10], Train Loss: 191101.2188, Train Accuracy: 0.7937, Train F1: 0.8497\n",
      "Val Loss: 39869.2891, Val Accuracy: 0.8599, Val F1: 0.8994\n",
      "Epoch [8/10], Train Loss: 188675.0469, Train Accuracy: 0.8044, Train F1: 0.8567\n",
      "Val Loss: 39224.9883, Val Accuracy: 0.8722, Val F1: 0.9070\n",
      "Epoch [9/10], Train Loss: 186551.7656, Train Accuracy: 0.8152, Train F1: 0.8638\n",
      "Val Loss: 38670.5859, Val Accuracy: 0.8879, Val F1: 0.9167\n",
      "Epoch [10/10], Train Loss: 184967.0156, Train Accuracy: 0.8290, Train F1: 0.8728\n",
      "Val Loss: 38228.6328, Val Accuracy: 0.9038, Val F1: 0.9266\n",
      "Test Loss: 341.3550, Test Accuracy: 0.8720, Test F1: 0.8962\n",
      "Accuracy: 0.9740\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.99       943\n",
      "         1.0       0.75      0.81      0.78        57\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.87      0.90      0.88      1000\n",
      "weighted avg       0.97      0.97      0.97      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "be = BinaryEnsembler(['Dos', 'Fuzzers'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda311new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
