{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shap\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "from explainerdashboard import ExplainerDashboard, ClassifierExplainer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import captum.attr as c\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_7820\\2584280520.py:1: DtypeWarning: Columns (1,3) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Cleaned_full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset retained index.\n",
    "data = data.reset_index(drop=True)\n",
    "# Set NA to 0.\n",
    "data['ct_ftp_cmd'] = data['ct_ftp_cmd'].fillna(0)\n",
    "data['attack_cat'] = data['attack_cat'].str.replace(r'\\s+', '', regex=True)\n",
    "data['attack_cat'] = data['attack_cat'].str.replace('Backdoors', 'Backdoor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoids scaling binaries.\n",
    "data = data.drop(columns=['proto', 'dsport', 'service', 'state', 'srcip', 'sport', 'dstip'])\n",
    "temp = data[['is_ftp_login', 'is_sm_ips_ports', 'label', 'attack_cat']]\n",
    "data = data.drop(columns=['is_ftp_login', 'is_sm_ips_ports', 'label', 'attack_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add encoded values for correlation encoder. The encoded values are based on all threats vs Normal labels.\n",
    "ohe1 = pd.read_csv('Full_proto_encoded.csv')\n",
    "ohe2 = pd.read_csv('Full_dsport_encoded.csv')\n",
    "ohe3 = pd.read_csv('Full_service_encoded.csv')\n",
    "ohe4 = pd.read_csv('Full_state_encoded.csv')\n",
    "# Spelling error.\n",
    "ohe5 = pd.read_csv('Full_scrip_encoded.csv')\n",
    "#------------------------------------------#\n",
    "ohe6 = pd.read_csv('Full_sport_encoded.csv')\n",
    "ohe7 = pd.read_csv('Full_dstip_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax seperates Normal data well and reduces noise. Please see Kmeans TSNE evaluation in Tools.\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "# Combine data after scaling.\n",
    "data = pd.concat([data, temp, ohe1, ohe2, ohe3, ohe4, ohe5, ohe6, ohe7], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    443831\n",
      "1     64179\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, data['label'], test_size=0.2, random_state=42)\n",
    "# Drop attack cat before running model and store for later evaluation indexing.\n",
    "test_attack_cat = X_test['attack_cat']\n",
    "print(X_test['label'].value_counts())\n",
    "# Drop labels from X sets pre training.\n",
    "X_train = X_train.drop(columns=['attack_cat', 'label'])\n",
    "X_test = X_test.drop(columns=['attack_cat', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batches.\n",
    "batch_size = 1024\n",
    "# Convert to tensors. BCE requires float type.\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# Add to Dataloader to manage batch processing.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "train_list = []\n",
    "label_list = []\n",
    "for batch_idx, (train_seq, train_label) in enumerate(train_loader):\n",
    "    batch_size = len(train_seq)\n",
    "    smote_neighbors = min(batch_size, 2)\n",
    "\n",
    "    if batch_size > 1:\n",
    "        try:\n",
    "            smote = SMOTE(random_state=42, k_neighbors=smote_neighbors)\n",
    "            train_smote, train_label_smote = smote.fit_resample(train_seq.numpy(), train_label.numpy())\n",
    "            train_smote = torch.tensor(train_smote, dtype=torch.float32)\n",
    "            train_label_smote = torch.tensor(train_label_smote, dtype=torch.float32)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error running SMOTE: {e}\")\n",
    "            train_smote, train_label_smote = train_seq, train_label\n",
    "    else:\n",
    "        train_smote, train_label_smote = train_seq, train_label\n",
    "    train_list.append(train_smote)\n",
    "    label_list.append(train_label_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985\n"
     ]
    }
   ],
   "source": [
    "print(len(train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = torch.cat(train_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3549866\n"
     ]
    }
   ],
   "source": [
    "print(len(train_full))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
