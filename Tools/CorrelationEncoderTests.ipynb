{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shap\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from explainerdashboard import InlineExplainer, ExplainerDashboard, ClassifierExplainer\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Encoders with fixed logic.\n",
    "Added a max_encoded logic along with some other encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrOnehotEncoder:\n",
    "    \"\"\"\n",
    "    CorrOnehotEncoder: Encodes the given column by creating one-hot encoded columns for categories that have\n",
    "    a correlation higher than a threshold with the target column.\n",
    "    \"\"\"\n",
    "    def __init__(self, column, target):\n",
    "        \"\"\"\n",
    "        Constructor: Stores the column and target (storing the full data causes memory issues).\n",
    "        \n",
    "        Parameters:\n",
    "            - column (pd.Series): The feature column to encode.\n",
    "            - target (pd.Series): The target column.\n",
    "        \"\"\"\n",
    "        # Force to string for groups.\n",
    "        self.column = column.astype(str)\n",
    "        # Convert to float32 precision to minimise memory load.\n",
    "        self.target = target.astype(np.float32)\n",
    "\n",
    "    def corr(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculate the Pearson correlation coefficient (Phi).\n",
    "        \n",
    "        Parameters:\n",
    "            - x (tensor - float32): The first variable.\n",
    "            - y (tensor - float32): The target to draw correlation to.\n",
    "        \n",
    "        Returns:\n",
    "            - r (float32): The Pearson correlation coefficient (Phi).\n",
    "        \"\"\"\n",
    "        mean_x = tf.reduce_mean(x)\n",
    "        mean_y = tf.reduce_mean(y)\n",
    "        covariance = tf.reduce_sum((x - mean_x) * (y - mean_y))\n",
    "        std_x = tf.sqrt(tf.reduce_sum((x - mean_x) ** 2))\n",
    "        std_y = tf.sqrt(tf.reduce_sum((y - mean_y) ** 2))\n",
    "        r = covariance / (std_x * std_y)\n",
    "        return r\n",
    "\n",
    "    def encode(self, sparse_n, threshold, max_encoded):\n",
    "        \"\"\"\n",
    "        Encode the feature column by creating one-hot encoded columns for categories that have\n",
    "        a correlation higher than a threshold with the target.\n",
    "        \n",
    "        Parameters:\n",
    "            - sparse_n (int): Minimum number of occurrences (1's) for a category in the column.\n",
    "            - threshold (float): The correlation threshold.\n",
    "            - max_encoded (int): The maximum number of encoded features.\n",
    "        \n",
    "        Returns:\n",
    "            - ohe_df (pd.DataFrame): One-hot encoded columns that meet the correlation threshold.\n",
    "        \"\"\"\n",
    "        # Convert to numpy for tensors.\n",
    "        column_np = self.column.to_numpy()\n",
    "        target_np = self.target.to_numpy()\n",
    "\n",
    "        # Store results.\n",
    "        ohe_list = []    \n",
    "        column_names = []\n",
    "        correlations = []\n",
    "        # Iterate through each unique category in the column.\n",
    "        for c in np.unique(column_np):\n",
    "            # Convert to binary - float32 minimises memory issues.\n",
    "            corr_column = (column_np == c).astype(np.float32)\n",
    "            # If the category count is below sparse_n, skip encoding.\n",
    "            if np.sum(corr_column) < sparse_n:\n",
    "                continue\n",
    "            # Convert to tensors for the correlation calculation.\n",
    "            correlation = self.corr(tf.convert_to_tensor(corr_column, dtype=tf.float32), \n",
    "                                    tf.convert_to_tensor(target_np, dtype=tf.float32))\n",
    "            # If the absolute correlation is greater than the threshold, add to the list.\n",
    "            if abs(correlation.numpy()) > threshold:\n",
    "                ohe_list.append(corr_column)\n",
    "                column_names.append(c)\n",
    "                # Store correlations to sort.\n",
    "                correlations.append(abs(correlation.numpy()))\n",
    "\n",
    "        # Sort the columns by their correlation with the target.\n",
    "        sorted_indices = np.argsort(correlations)[::-1]\n",
    "        sorted_ohe_list = []\n",
    "        sorted_column_names = []\n",
    "        for i in sorted_indices:\n",
    "            sorted_ohe_list.append(ohe_list[i])\n",
    "            sorted_column_names.append(column_names[i])\n",
    "\n",
    "        # Limit the number of variables to max_encoded.\n",
    "        if len(sorted_ohe_list) > max_encoded:\n",
    "            sorted_ohe_list = sorted_ohe_list[:max_encoded]\n",
    "            sorted_column_names = sorted_column_names[:max_encoded]\n",
    "        # Add the encoded data to a dataframe.\n",
    "        ohe_df = pd.DataFrame(np.column_stack(sorted_ohe_list), columns=sorted_column_names)\n",
    "        \n",
    "        if ohe_df.empty:\n",
    "            print(\"No correlations exceed the threshold.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return ohe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrVarEncoder:\n",
    "    \"\"\"\n",
    "    CorrThresholdEncoder: Encodes a given column based on a correlation threshold. All values within the variable that fall below the threshold are\n",
    "    converted to a given string name. For Example: LowThreshold.\n",
    "    \n",
    "    NOTE: It is recommended to include the threshold used in the new value name.\n",
    "\n",
    "    Initialisation:\n",
    "        - data (pd.Series): The column that contains the feature.\n",
    "        - target (pd.Series): The target column to draw correlation to.\n",
    "    \"\"\"\n",
    "    def __init__(self, column, target):\n",
    "        self.column = column.astype(str)\n",
    "        self.target = target.astype(np.float32)\n",
    "\n",
    "    def corr(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculate the Pearson correlation coefficient (Phi).\n",
    "        \n",
    "        Parameters:\n",
    "            - x (tensor - float32): The first variable.\n",
    "            - y (tensor - float32): The target to draw correlation to.\n",
    "        \n",
    "        Returns:\n",
    "            - r (float32): The Pearson correlation coefficient (Phi).\n",
    "        \"\"\"\n",
    "        mean_x = tf.reduce_mean(x)\n",
    "        mean_y = tf.reduce_mean(y)\n",
    "        covariance = tf.reduce_sum((x - mean_x) * (y - mean_y))\n",
    "        std_x = tf.sqrt(tf.reduce_sum((x - mean_x) ** 2))\n",
    "        std_y = tf.sqrt(tf.reduce_sum((y - mean_y) ** 2))\n",
    "        r = covariance / (std_x * std_y)\n",
    "        return r\n",
    "\n",
    "    def encode(self, threshold, value_name, sparse_n, max_encoded):\n",
    "        \"\"\"\n",
    "        encode: Takes the column to encode and computes the correlation with the target column. If the correlation is below the threshold, \n",
    "        the category value is replaced with a specified value name. It also filters categories based on the sparse_n condition.\n",
    "        \n",
    "        Parameters:\n",
    "            - threshold (float): The threshold for correlation. The function creates onehot encoded columns of all variables that have correlation\n",
    "              higher than the threshold to the target label.\n",
    "            - value_name (str): The value to replace categories with low correlation.\n",
    "            - sparse_n (int): The minimum number of occurrences for a category to be considered.\n",
    "            - max_encoded (int): The maximum number of categories.\n",
    "        \n",
    "        Returns:\n",
    "            - pd.Series: The converted column.\n",
    "        \"\"\"\n",
    "        corr_dict = {}\n",
    "\n",
    "        # Go through each unique category in the column.\n",
    "        for c in self.column.unique():\n",
    "            corr_column = (self.column == c).astype(np.float32)\n",
    "            num_ones = corr_column.sum()\n",
    "            # Set category to value name if below sparse_n.\n",
    "            if num_ones < sparse_n:\n",
    "                self.column[self.column == c] = value_name\n",
    "                continue\n",
    "\n",
    "            # Convert to tensors to minimise memory allocation.\n",
    "            corr_column_tensor = tf.convert_to_tensor(corr_column, dtype=tf.float32)\n",
    "            target_tensor = tf.convert_to_tensor(self.target, dtype=tf.float32)\n",
    "            # Calculate the correlation with the target label.\n",
    "            correlation = self.corr(corr_column_tensor, target_tensor)\n",
    "            # Only add to the dictionary if the correlation is above the threshold.\n",
    "            if abs(correlation.numpy()) >= threshold:\n",
    "                corr_dict[c] = correlation.numpy()\n",
    "            else:\n",
    "                # If correlation is below threshold, mark as low correlation.\n",
    "                self.column[self.column == c] = value_name\n",
    "\n",
    "        # Sort categories for max_encoded.\n",
    "        sorted_corr_dict = sorted(corr_dict.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        limited_categories = []\n",
    "        for item in sorted_corr_dict[:min(max_encoded-1, len(sorted_corr_dict))]:\n",
    "            limited_categories.append(item[0])\n",
    "\n",
    "        # Replace values that are not in the top 'max_encoded' categories.\n",
    "        for c in self.column.unique():\n",
    "            if c not in limited_categories:\n",
    "                self.column[self.column == c] = value_name\n",
    "\n",
    "        return self.column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrBinEncoder:\n",
    "    \"\"\"\n",
    "    CorrBinEncoder: Encodes a variable based on the correlation drawn to the given label based on the number of categories provided. The variable is binarised\n",
    "    using the correlations with pd.cut.\n",
    "\n",
    "    NOTE: pd.cut creates relative borders when binarising which means that at times even a value that is labelled as High might still only be low correlation (0.1).\n",
    "    \"\"\"\n",
    "    def __init__(self, column, target):\n",
    "        self.column = column.astype(str)\n",
    "        self.target = target.astype(np.float32)\n",
    "\n",
    "    def corr(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculate the Pearson correlation (Phi).\n",
    "        \n",
    "        Parameters:\n",
    "            - x (tensor - float32): The first variable.\n",
    "            - y (tensor - float32): The target to draw correlation to.\n",
    "        \n",
    "        Returns:\n",
    "            - r (float32): The Pearson correlation coefficient (Phi).\n",
    "        \"\"\"\n",
    "        mean_x = tf.reduce_mean(x)\n",
    "        mean_y = tf.reduce_mean(y)\n",
    "        covariance = tf.reduce_sum((x - mean_x) * (y - mean_y))\n",
    "        std_x = tf.sqrt(tf.reduce_sum((x - mean_x) ** 2))\n",
    "        std_y = tf.sqrt(tf.reduce_sum((y - mean_y) ** 2))\n",
    "        r = covariance / (std_x * std_y)\n",
    "        return r\n",
    "\n",
    "    def encode(self, column, bin_cut, bin_labels):\n",
    "        \"\"\"\n",
    "        encode: Select a number of bins and corresponding labels to binarize the variable based on correlation to the label.\n",
    "\n",
    "        Parameters:\n",
    "            - column (string): The column to encode.\n",
    "            - bin_cut (int): The number of bins to create based on pd.cut.\n",
    "            - bin_labels (list): A list of strings to name the new values (High, Medium, Low) - must match the same number of bins.\n",
    "\n",
    "        Returns:\n",
    "            - pd.Series: The encoded column with correlation binned by categories.\n",
    "        \"\"\"\n",
    "        corr_dict = {}\n",
    "\n",
    "        # Go through each unique category in the column.\n",
    "        for c in self.column.unique():\n",
    "            # Create a binary column for each category.\n",
    "            corr_column = (self.column == c).astype(np.float32)\n",
    "\n",
    "            # Convert to tensors to minimise memory for corr calculation.\n",
    "            corr_column_tensor = tf.convert_to_tensor(corr_column, dtype=tf.float32)\n",
    "            target_tensor = tf.convert_to_tensor(self.target, dtype=tf.float32)\n",
    "            correlation = self.corr(corr_column_tensor, target_tensor)\n",
    "            corr_dict[c] = correlation.numpy()\n",
    "\n",
    "        # Create a DataFrame for cut.\n",
    "        corr_df = pd.DataFrame(list(corr_dict.items()), columns=['category', 'corr'])\n",
    "        corr_df['abs_corr'] = corr_df['corr'].abs()\n",
    "        # Binarise using pd.cut.\n",
    "        corr_df['binned'] = pd.cut(\n",
    "            corr_df['abs_corr'],\n",
    "            bins=bin_cut,\n",
    "            labels=bin_labels,\n",
    "            include_lowest=True\n",
    "        )\n",
    "        # Map each category to its corresponding bin.\n",
    "        category_to_bin = dict(zip(corr_df['category'], corr_df['binned']))\n",
    "        encoded_column = self.column.map(category_to_bin)\n",
    "\n",
    "        return encoded_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Version:\n",
    "- kept as a reference for the original DosAnalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrEncoder:\n",
    "    \"\"\"\n",
    "    CorrEncoder: Takes a dataset as input and uses it for the encode function. Encodes the filtered categories then draws correlations.\n",
    "    If correlation is above the threshold adds it to a new dataframe then returns the one hot encoded values with the labels.\n",
    "\n",
    "    Initialisation:\n",
    "        - data (pd.DataFrame): The Dataset that contains the target column and target label variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data.reset_index(drop=True).copy()\n",
    "        #self.data = self.data.drop(columns=['Label'])\n",
    "\n",
    "    def encode(self, target_column, sparse_n, threshold):\n",
    "        \"\"\"\n",
    "        encode: Takes a target column and target label to encode and draw correlations from. The target column is iterated through\n",
    "        for all categories that contain more positive values than defined in sparse_n. This allows for filtering of sparse categories.\n",
    "        The function then one hot encodes the given category with the static target column and draws correlations for them. If correlation\n",
    "        is greater then threshold then add it to the new DataFrame. The function returns the one hot encoded categories that pass the\n",
    "        threshold with the target label.\n",
    "\n",
    "        The purpose of this function is to resolve the high cardinality problem in one hot encoding.\n",
    "\n",
    "        Parameters:\n",
    "            - target_column (string): The name of the target column. The target column should contain the various categories to encode.\n",
    "            - sparse_n (integer): The minimum amount of positive values required for a category after encoding (deals with sparse categories).\n",
    "            - threshold (float): The threshold for correlation. The function creates onehot encoded columns of all variables that have correlation\n",
    "              higher than the threshold to the target label.\n",
    "\n",
    "        Returns:\n",
    "            - ohe_df (pd.DataFrame): The one hot encoded values from the target columns.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.data[target_column] = self.data[target_column].astype(str)\n",
    "        value_counts = self.data[target_column].value_counts()\n",
    "        # Check if number of 1s is above the given threshold set by sparse_n.\n",
    "        categories = value_counts[value_counts > sparse_n].index.tolist()\n",
    "        ohe_list = []    \n",
    "        attack_cat = self.data['attack_cat']\n",
    "        # Go through each unique category in the target column.\n",
    "        for c in categories:\n",
    "            col_name = f'{target_column}_{c}'\n",
    "\n",
    "            # Create the binary encoding column for the current category and target label\n",
    "            corr_column = (self.data[target_column] == c).astype(int)\n",
    "            correlation = corr_column.corr(attack_cat)\n",
    "\n",
    "            # Check if absolute correlation is greater than threshold.\n",
    "            if abs(correlation) > threshold:\n",
    "                corr_column.name = col_name\n",
    "                ohe_list.append(corr_column)\n",
    "        print('Number of Encoded Features for', target_column)\n",
    "        print(len(ohe_list))\n",
    "        if ohe_list:\n",
    "            # NOTE: This section can be expanded to include print outs but at the moment am focusing on the evaluations.\n",
    "            ohe_df = pd.concat(ohe_list, axis=1)\n",
    "            return ohe_df\n",
    "        else:\n",
    "            # This ommits errors (if really high thresholds are used).\n",
    "            print(\"No correlations exceed the threshold.\")\n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_16736\\112763362.py:1: DtypeWarning: Columns (1,3) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../Cleaned_full_data.csv')\n",
    "data = data.reset_index(drop=True)\n",
    "# Set NA to 0.\n",
    "data['ct_ftp_cmd'] = data['ct_ftp_cmd'].fillna(0)\n",
    "data['ct_ftp_cmd'] = data['ct_ftp_cmd'].fillna(0)\n",
    "data['attack_cat'] = data['attack_cat'].str.replace(r'\\s+', '', regex=True)\n",
    "data['attack_cat'] = data['attack_cat'].str.replace('Backdoors', 'Backdoor')\n",
    "# Select a threat category.\n",
    "category = 'DoS'\n",
    "data['attack_cat'] = (data['attack_cat'] == category).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encoded Features for proto\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "encoder = CorrEncoder(data)\n",
    "threshold = 0.01\n",
    "ohe4a = encoder.encode('proto', 30, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'dsport': 128310\n",
      "Column 'proto': 135\n",
      "Column 'sport': 100341\n",
      "Column 'srcip': 43\n",
      "Column 'dstip': 47\n"
     ]
    }
   ],
   "source": [
    "# Although Dsport has just as many unique categories it is not as sensitive to threshold as sport.\n",
    "encoded_columns = ['dsport', 'proto', 'sport', 'srcip', 'dstip', ]\n",
    "for column in encoded_columns:\n",
    "    unique_categories = data[column].nunique()\n",
    "    print(f\"Column '{column}': {unique_categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encoded Features for sport\n",
      "11\n",
      "Number of Encoded Features for state\n",
      "9\n",
      "Number of Encoded Features for service\n",
      "13\n",
      "Number of Encoded Features for proto\n",
      "132\n",
      "Number of Encoded Features for dsport\n",
      "76\n",
      "Number of Encoded Features for srcip\n",
      "26\n",
      "Number of Encoded Features for dstip\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# Takes around 90 minutes (likely because many values are just above sparse_n in sport\n",
    "# column considering the attack type).\n",
    "encoder = CorrEncoder(data)\n",
    "threshold = 0.01\n",
    "# 0.001 fails with memory allocation for sport (concatenating the columns).\n",
    "# Could try 0.005.\n",
    "ohe1a = encoder.encode('sport', 30, threshold)\n",
    "threshold = 0\n",
    "ohe2 = encoder.encode('state', 30, threshold)\n",
    "ohe3 = encoder.encode('service', 30, threshold)\n",
    "ohe4 = encoder.encode('proto', 30, threshold)\n",
    "# This could be reduced to 0 too but just want to see if we get an improvement with the current\n",
    "# settings.\n",
    "threshold = 0.001\n",
    "ohe5 = encoder.encode('dsport', 30, threshold)\n",
    "ohe6 = encoder.encode('srcip', 30, threshold)\n",
    "ohe7 = encoder.encode('dstip', 30, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoS_proto2_encoded.csv\n"
     ]
    }
   ],
   "source": [
    "encoded_columns = {\n",
    "    'proto2': ohe4a,\n",
    "}\n",
    "\n",
    "# Save data. This is the most optimal set I have been able to get without memory issues.\n",
    "# NOTE: Batching may not be possible because we won't be drawing correlation on the full variable.\n",
    "for column_name, encoded_data in encoded_columns.items():\n",
    "    encoded_data.to_csv(f'DoS_{column_name}_encoded.csv', index=False)\n",
    "    print(f\"DoS_{column_name}_encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = pd.concat([data, ohe1a, ohe2, ohe3, ohe4, ohe5, ohe6, ohe7], axis=1)\n",
    "data_encoded = data_encoded.drop(columns=['sport', 'state', 'service', 'proto', 'dsport', 'srcip', 'dstip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n"
     ]
    }
   ],
   "source": [
    "print(len(data_encoded.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
