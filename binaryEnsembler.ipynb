{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Ensembler:\n",
    "The Binary Ensembler Trains multiple LSTM and Random Forests and stores them in an object. The code preprocesses a dataset for each of the given labels based on a correlation threshold (only applied to categorical features currently). Then it trains a LSTM and Random Forest on each set and prints the evaluations for each label. Has options for \n",
    "a downsampled dataset and the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "import os\n",
    "import time\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import random\n",
    "import shap\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train, val, epochs, title):\n",
    "    \"\"\"\n",
    "    Plot the training, validation, and test metrics.\n",
    "\n",
    "    Parameters:\n",
    "        - train_loss (list): Training metric.\n",
    "        - val_loss (list): Validation metric.\n",
    "        - test_loss (list): Test tmetric.\n",
    "        - epochs (int): The number of epochs.\n",
    "        - title (string): The title of the plot (imputes the category with type of plot).\n",
    "    \"\"\"\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(range(1, epochs + 1), train, label='Training', color=sns.color_palette(\"pastel\")[0])\n",
    "    plt.plot(range(1, epochs + 1), val, label='Validation', color=sns.color_palette(\"pastel\")[1])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(f\"{title}\")\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.legend()\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrEncoder:\n",
    "    \"\"\"\n",
    "    CorrEncoder: Takes a dataset as input and uses it for the encode function. Encodes the filtered categories then draws correlations.\n",
    "    If correlation is above the threshold adds it to a new dataframe then returns the one hot encoded values with the labels.\n",
    "\n",
    "    Initialisation:\n",
    "        - data (pd.DataFrame): The Dataset that contains the target column and target label variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data.copy()\n",
    "        # Removes Label for the binary processing as it is based on the specific attack_cat.\n",
    "        self.data = self.data.drop(columns=['Label'])\n",
    "\n",
    "    def encode(self, target_column, sparse_n, threshold, cat):\n",
    "        \"\"\"\n",
    "        encode: Takes a target column and target label to encode and draw correlations from. The target column is iterated through\n",
    "        for all categories that contain more positive values than defined in sparse_n. This allows for filtering of sparse categories.\n",
    "        The function then one hot encodes the given category with the static target column and draws correlations for them. If correlation\n",
    "        is greater then threshold then add it to the new DataFrame. The function returns the one hot encoded categories that pass the\n",
    "        threshold with the target label.\n",
    "\n",
    "        The purpose of this function is to resolve the high cardinality problem in one hot encoding.\n",
    "\n",
    "        Parameters:\n",
    "            - target_column (string): The name of the target column. The target column should contain the various categories to encode.\n",
    "            - sparse_n (integer): The minimum amount of positive values required for a category after encoding (deals with sparse categories).\n",
    "            - threshold (float): The threshold for correlation. The function creates onehot encoded columns of all variables that high correlation\n",
    "              higher that the threshold to the target label.\n",
    "            - cat (string): The category label to compare to.\n",
    "\n",
    "        Returns:\n",
    "            - ohe_df (pd.DataFrame): The one hot encoded values from the target columns.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to string to handle duplicates.\n",
    "        self.data[target_column] = self.data[target_column].astype(str)\n",
    "        value_counts = self.data[target_column].value_counts()\n",
    "        # Check if number of 1s is above the given threshold set by sparse_n.\n",
    "        categories = value_counts[value_counts > sparse_n].index.tolist()\n",
    "        ohe_list = []\n",
    "\n",
    "        # Convert the target label to one hot encoded.\n",
    "        attack_cat = (self.data['attack_cat'] == cat).astype(int)\n",
    "        # Go through each unique category in the target column.\n",
    "        for c in categories:\n",
    "            col_name = f'{target_column}_{c}'\n",
    "\n",
    "            # Create the binary encoding column for the current category and target label\n",
    "            corr_column = (self.data[target_column] == c).astype(int)\n",
    "            correlation = corr_column.corr(attack_cat)\n",
    "\n",
    "            # Check if absolute correlation is greater than threshold.\n",
    "            if abs(correlation) > threshold:\n",
    "                corr_column.name = col_name\n",
    "                ohe_list.append(corr_column)\n",
    "        if ohe_list:\n",
    "            # NOTE: This section can be expanded to include print outs but at the moment am focusing on the evaluations.\n",
    "            ohe_df = pd.concat(ohe_list, axis=1)\n",
    "            return ohe_df\n",
    "        else:\n",
    "            # This ommits errors (if really high thresholds are used).\n",
    "            print(\"No correlations exceed the threshold.\")\n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSamplerBinary:\n",
    "    \"\"\"\n",
    "    DataSamplerBinary3: Preprocesses data for the LSTM and Random Forest for a given label category. Uses down-sampling with a mask to reduce the Normal labels to Generic for only\n",
    "    the values that don't precede a threat row. Alternatively uses the full data. Preprocesses and encodes the categories by filter categories with correlation below a given threshold. Creates tensors for the LSTM\n",
    "    and stores the data for referencing through the ensembler.\n",
    "\n",
    "    Initialisation:\n",
    "        - train (None): The attribute that stores the train tensor.\n",
    "        - val(None): The attribute that stores the train tensor.\n",
    "        - test (None): The attribute that stores the train tensor.\n",
    "        - name (string): The name of the label category.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.test = None\n",
    "        self.name = None\n",
    "\n",
    "    def get_data(self, type_of, size, rs, category, threshold):\n",
    "        \"\"\"\n",
    "        get_data: Preprocess and transform data for the LSTM and Random Forest. Combines the 4 USNW-NB15 datasets and downsamples the Normal labels to Generic labels using a mask\n",
    "        to retain all values that precede a threat row. Alternatively uses all the data.One hot encodes the categories using a given threshold for correlation through the CorrEncoder class. Scales the data and then\n",
    "        splits a train, validation, and test set using slicing. Finally creates and returns tensors with an added dimension for the LSTM. The class stores the datasets as attributes\n",
    "        for referencing.\n",
    "\n",
    "        Parameters:\n",
    "            - data (string): The type of preprocessing to use (currently only has 'downsample')\n",
    "            - size (float): The proportion for the validation and test set (same size)\n",
    "            - rs (int): The random seed to use for the slicing.\n",
    "            - category (string): The label category to preprocess.\n",
    "            - threshold (float): The threshold value for the minimum correlation in one hot encoded categories.\n",
    "        \n",
    "        Returns:\n",
    "            - train_dataset (tensor): The training tensor given as (rows, sequence, features).\n",
    "            - val_dataset (tensor): The validation tensor given as (rows, sequence, features).\n",
    "            - test_dataset (tensor): The testing tensor given as (rows, sequence, features).        \n",
    "        \"\"\"\n",
    "\n",
    "        # Combine datasets and downsample.\n",
    "        # NOTE: The null values are converted to Normal labels.\n",
    "        feature_names = pd.read_csv('features2.csv')\n",
    "        self.name = category\n",
    "        feature_names_list = feature_names['Name'].tolist()\n",
    "        s1 = pd.read_csv('UNSW-NB15_1.csv', header=None)\n",
    "        s1.columns = feature_names_list\n",
    "        s1.loc[s1['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "\n",
    "        feature_names_list = feature_names['Name'].tolist()\n",
    "        s2 = pd.read_csv('UNSW-NB15_2.csv', header=None)\n",
    "        s2.columns = feature_names_list\n",
    "        s2.loc[s2['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "\n",
    "        feature_names_list = feature_names['Name'].tolist()\n",
    "        s3 = pd.read_csv('UNSW-NB15_3.csv', header=None)\n",
    "        s3.columns = feature_names_list\n",
    "        s3.loc[s3['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "\n",
    "        feature_names_list = feature_names['Name'].tolist()\n",
    "        s4 = pd.read_csv('UNSW-NB15_4.csv', header=None)\n",
    "        s4.columns = feature_names_list\n",
    "        s4.loc[s4['attack_cat'].isnull(), 'attack_cat'] = 'Normal'\n",
    "        \n",
    "        i = 0\n",
    "        data = [s1, s2, s3, s4]\n",
    "        # If downsample is chosen, downsample the Normal labels to Generic labels for each set individually.\n",
    "        if type_of == 'downsample':\n",
    "            while i < len(data):\n",
    "                df = data[i]\n",
    "                normal = df[df['attack_cat'] == 'Normal'].shape[0]\n",
    "                generic = df[df['attack_cat'] == 'Generic'].shape[0]\n",
    "                difference = normal - generic\n",
    "                mask = df['Label'].shift(-1) != 1\n",
    "                rows = df[(df['attack_cat'] == 'Normal') & mask]\n",
    "                downsampled = rows.sample(n=difference, random_state=rs)\n",
    "                df2 = df.drop(downsampled.index)\n",
    "                data[i] = df2\n",
    "                i += 1\n",
    "\n",
    "        # Combine the datasets\n",
    "        full_data = pd.concat([data[0], data[1], data[2], data[3]]).reset_index(drop=True)\n",
    "\n",
    "        # Clean Labels and drop sparse data.\n",
    "        full_data['attack_cat'] = full_data['attack_cat'].str.replace(r'\\s+', '', regex=True)\n",
    "        full_data['attack_cat'] = full_data['attack_cat'].str.replace('Backdoors', 'Backdoor')\n",
    "        full_data = full_data.drop(columns=['ct_ftp_cmd', 'ct_flw_http_mthd', 'is_ftp_login'])\n",
    "        # Clean error values (imported as hex strings for some reason).\n",
    "        full_data = full_data[~full_data['sport'].astype(str).str.startswith('0x')]\n",
    "        full_data = full_data[~full_data['sport'].astype(str).str.startswith('-')]\n",
    "        full_data['sport'] = full_data['sport'].apply(pd.to_numeric)\n",
    "        full_data = full_data[~full_data['dsport'].astype(str).str.startswith('0x')]\n",
    "        full_data = full_data[~full_data['dsport'].astype(str).str.startswith('-')]\n",
    "        full_data['dsport'] = full_data['dsport'].apply(pd.to_numeric)\n",
    "\n",
    "        columns = ['state', 'service']\n",
    "        encoder = OneHotEncoder(sparse_output=False, dtype='float32')\n",
    "        encoded_data = encoder.fit_transform(full_data[columns])\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns), index=full_data.index)\n",
    "        full_encoded = pd.concat([full_data.drop(columns=columns), encoded_df], axis=1)\n",
    "\n",
    "        # Run CorrEncoder on remaining categorical features.\n",
    "        encoder = CorrEncoder(full_encoded)\n",
    "        ohe1 = encoder.encode('dsport', 30, threshold, category)\n",
    "        ohe2 = encoder.encode('proto', 30, threshold, category)\n",
    "        ohe3 = encoder.encode('sport', 30, threshold, category)\n",
    "        ohe4 = encoder.encode('srcip', 30, threshold, category)\n",
    "        ohe5 = encoder.encode('dstip', 30, threshold, category)\n",
    "\n",
    "        # Drop the original columns.\n",
    "        cols_to_drop = ['dsport', 'proto', 'sport', 'srcip', 'dstip']\n",
    "        filtered_data = full_encoded.drop(columns=cols_to_drop)\n",
    "        combined_data = pd.concat([filtered_data, ohe1, ohe2, ohe3, ohe4, ohe5], axis=1)\n",
    "        # NOTE: There is no need for LabelEncoder here as we know threat = 1 and rest = 0.\n",
    "        #encoder = LabelEncoder()\n",
    "        combined_data['attack_cat'] = (combined_data['attack_cat'] == category).astype(int)\n",
    "        \n",
    "        # Maybe StandardScaler might be more appropriate but not sure how it may look on the graph (SHAP).\n",
    "        df_features = combined_data.drop(columns=['attack_cat', 'Label'])\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_data = scaler.fit_transform(df_features)\n",
    "        final_data = pd.DataFrame(scaled_data, columns=df_features.columns, index=combined_data.index)\n",
    "        # Ommit adding 'Label' column back in.\n",
    "        final_data['attack_cat'] = combined_data['attack_cat']\n",
    "\n",
    "        # Slice the data for the training, validation, and test sets.\n",
    "        slice_size = int(size * len(final_data))\n",
    "        val_start = random.randrange(0, len(final_data) - 2 * slice_size)\n",
    "        val_end = val_start + slice_size\n",
    "        val_data = final_data.iloc[val_start:val_end]\n",
    "        df = final_data.drop(val_data.index)\n",
    "        test_start = random.randrange(0, len(df) - slice_size)\n",
    "        test_end = test_start + slice_size\n",
    "        test_data = df.iloc[test_start:test_end]\n",
    "        train_data = df.drop(test_data.index)\n",
    "\n",
    "        # Create tensors with added dimensions.\n",
    "        # NOTE: BinaryLSTM requires float32 type.\n",
    "        train_seq = torch.tensor(train_data.drop(columns=['attack_cat']).values, dtype=torch.float32)\n",
    "        val_seq = torch.tensor(val_data.drop(columns=['attack_cat']).values, dtype=torch.float32)\n",
    "        test_seq = torch.tensor(test_data.drop(columns=['attack_cat']).values, dtype=torch.float32)\n",
    "\n",
    "        train_label = torch.tensor(train_data['attack_cat'].values, dtype=torch.float32)\n",
    "        val_label = torch.tensor(val_data['attack_cat'].values, dtype=torch.float32)\n",
    "        test_label = torch.tensor(test_data['attack_cat'].values, dtype=torch.float32)\n",
    "            \n",
    "        # Add dimension.\n",
    "        train_seq = train_seq.unsqueeze(1)\n",
    "        val_seq = val_seq.unsqueeze(1)\n",
    "        test_seq = test_seq.unsqueeze(1)\n",
    "\n",
    "        train_dataset = TensorDataset(train_seq, train_label)\n",
    "        val_dataset = TensorDataset(val_seq, val_label)\n",
    "        test_dataset = TensorDataset(test_seq, test_label)\n",
    "        # Store as attributes for easy referencing.\n",
    "        self.train = train_dataset\n",
    "        self.val = val_dataset\n",
    "        self.test = test_dataset\n",
    "\n",
    "        return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    BinaryLSTM: A Binary LSTM nn.Module. Trains and validates the model, then tests it and stores the evaluation metrics as attributes.\n",
    "\n",
    "    Initialisation:\n",
    "        - input_size (integer): The number of features.\n",
    "        - hidden_size (integer): The hidden layer size (increases complexity)\n",
    "        - num_layers (integer): The number of LSTM layers.\n",
    "        - device (torch.device): The type of device used for computing (currently cpu due to errors with SHAP). \n",
    "        - save_dir (string): The directory file name for the model weights (needs to be defined properly). \n",
    "        - name (string): The name of the label category assigned to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, device, save_dir, name):\n",
    "        super(BinaryLSTM, self).__init__()\n",
    "        self.name = name\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.device = device\n",
    "        self.save_dir = save_dir\n",
    "        self.to(self.device)\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.train_f1 = []\n",
    "        self.val_f1 = []\n",
    "        self.epoch_time = []\n",
    "        self.test_accuracy = None\n",
    "        self.test_loss = None\n",
    "        self.test_f1 = None\n",
    "        self.test_predicted = []\n",
    "        self.test_actual = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward: The forward pass for the LSTM. Calculates the label for the same row.\n",
    "        This is a magic method called when the model is called self(input).\n",
    "\n",
    "        Parameters:\n",
    "            - x (tensor): The input sequence for the LSTM.\n",
    "\n",
    "        Returns:\n",
    "            - output (tensor): Returns the output after the LSTM layer.\n",
    "        \"\"\"\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        final_hidden_state = lstm_out[:, -1, :]\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "    def calculate_accuracy_f1(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        calculate_accuracy_f1: A helper function to help modularize the code. Calculates accuracy and\n",
    "        f1 macro (average).\n",
    "\n",
    "        Parameters:\n",
    "            - predictions (tensor): The predictions given by BCEWithLogitsLoss()\n",
    "            - labels (tensor): The actual labels of the test data.\n",
    "\n",
    "        Returns:\n",
    "            - accuracy (float): The accuracy score.\n",
    "            - f1 (float): The macro f1 score.\n",
    "        \"\"\"\n",
    "        preds = torch.sigmoid(predictions)\n",
    "        preds = (preds > 0.5).float()\n",
    "        accuracy = accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1 = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
    "        return accuracy, f1\n",
    "\n",
    "    def run(self, train_data, val_data, test_data, criterion, optimizer, epochs, save_factor):\n",
    "        \"\"\"\n",
    "        run: Run the model on the train, val data for each epoch. Evaluate on the test data at the end\n",
    "        of all epochs. Returns an explainer object based on the gradient of all datapoints in the test set and model.\n",
    "\n",
    "        Parameters:\n",
    "            - train_data (TensorDataset): The tensor holding the train features and labels.\n",
    "            - val_data (TensorDataset): The tensor holding the validation features and labels.\n",
    "            - test_data (TensorDataset): The tensor holding the test features and labels.\n",
    "            - criterion (nn.BCEWithLogitsLoss): The probability calculation for logits.\n",
    "            - optimizer (optim.Adam): The automatic regularization optimizer.\n",
    "            - epochs (int): The number of epochs to train.\n",
    "            - save_factor (int): The factor at which to save the model during training.\n",
    "\n",
    "        Returns:\n",
    "            - explainer (shap.GradientExplainer): The calculated explainer for all data on the\n",
    "            test set.\n",
    "        \"\"\"\n",
    "        # Split the tensors.\n",
    "        train_seq, train_label = train_data.tensors\n",
    "        val_seq, val_label = val_data.tensors\n",
    "        test_seq, test_label = test_data.tensors\n",
    "        train_seq, train_label = train_seq.to(self.device), train_label.to(self.device)\n",
    "        val_seq, val_label = val_seq.to(self.device), val_label.to(self.device)\n",
    "        test_seq, test_label = test_seq.to(self.device), test_label.to(self.device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass.\n",
    "            outputs = self(train_seq)\n",
    "            loss = criterion(outputs.squeeze(), train_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_accuracy, train_f1 = self.calculate_accuracy_f1(outputs, train_label)\n",
    "\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = self(val_seq)\n",
    "                val_loss = criterion(val_outputs.squeeze(), val_label)\n",
    "                val_accuracy, val_f1 = self.calculate_accuracy_f1(val_outputs, val_label)\n",
    "            self.train_loss.append(loss.item())\n",
    "            self.val_loss.append(val_loss.item())\n",
    "            self.train_accuracy.append(train_accuracy)\n",
    "            self.val_accuracy.append(val_accuracy)\n",
    "            self.train_f1.append(train_f1)\n",
    "            self.val_f1.append(val_f1)\n",
    "            epoch_time = time.time() - start_time\n",
    "            self.epoch_time.append(epoch_time)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "            if (epoch + 1) % save_factor == 0:\n",
    "                self.save_model(epoch + 1)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = self(test_seq)\n",
    "            test_loss = criterion(test_outputs.squeeze(), test_label)\n",
    "            test_accuracy, test_f1 = self.calculate_accuracy_f1(test_outputs, test_label)\n",
    "            self.test_accuracy = test_accuracy\n",
    "            self.test_loss = test_loss.item()\n",
    "            self.test_f1 = test_f1\n",
    "\n",
    "            self.test_pred = (torch.sigmoid(test_outputs) > 0.5).cpu().numpy()\n",
    "            self.test_actual = test_label.cpu().numpy()\n",
    "            print(f\"Test Loss: {self.test_loss:.4f}, Test Accuracy: {self.test_accuracy:.4f}, Test F1: {self.test_f1:.4f}\")\n",
    "\n",
    "        # NOTE: We cannot use no grad with SHAP explainers due to conflicts.\n",
    "        explainer = shap.GradientExplainer(self, test_seq)\n",
    "        return explainer\n",
    "\n",
    "    def save_model(self, epoch):\n",
    "        \"\"\"\n",
    "        save_model: Save the model state_dict for the given factor. Approx 1 megabyte each.\n",
    "\n",
    "        Parameters:\n",
    "            - The current epoch that is being saved.\n",
    "        \"\"\"\n",
    "        checkpoint_path = os.path.join(self.save_dir, f\"lstm_epoch_{epoch}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    def load_model(self, epoch):\n",
    "        \"\"\"\n",
    "        load_model: Load a state_dict of a model.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch to load.\n",
    "        \"\"\"\n",
    "        checkpoint_path = os.path.join(self.save_dir, f\"lstm_epoch_{epoch}.pt\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryEnsembler:\n",
    "    \"\"\"\n",
    "    BinaryEnsembler: Creates an ensemble of Binary LSTM and Binary Random Forests and stores them as attributes along with the given data. The models are stored as attributes\n",
    "    allowing referencing to the models attributes.\n",
    "\n",
    "    Initialisation:\n",
    "        - \n",
    "    \"\"\"\n",
    "    def __init__(self, labels, hidden_size, num_layers, epochs, threshold, learning_rate):\n",
    "        self.labels = labels\n",
    "        self.data_ensemble = []\n",
    "        self.model_ensemble = []\n",
    "        self.explainers_ensemble = []\n",
    "        self.tree_ensemble = []\n",
    "        for label in self.labels:\n",
    "            ds = DataSamplerBinary()\n",
    "            train_data, val_data, test_data = ds.get_data('downsample', 0.15, 42, label, threshold)\n",
    "            self.data_ensemble.append(ds)\n",
    "            device = torch.device(\"cpu\")\n",
    "            input_size = train_data.tensors[0].shape[2]\n",
    "            model = BinaryLSTM(input_size, hidden_size, num_layers, device, 'model', label)\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            explainer = model.run(train_data, val_data, test_data, criterion, optimizer, epochs, 50)\n",
    "            self.model_ensemble.append(model)\n",
    "            self.explainers_ensemble.append(explainer)\n",
    "            # All metrics are held within the model and can be called.\n",
    "            plot_metrics(model.train_loss, model.val_loss, epochs, label + ' Loss')\n",
    "            plot_metrics(model.train_f1, model.val_f1, epochs, label + ' F1')\n",
    "            plot_metrics(model.train_accuracy, model.val_accuracy, epochs, label + ' Accuracy')\n",
    "            # There is only 1 time but omits errors if wanting to just plot 1.\n",
    "            plot_metrics(model.epoch_time, model.epoch_time, epochs, label + ' Time')\n",
    "            tr_data, labels = train_data.tensors\n",
    "            tr_data = tr_data.squeeze()\n",
    "            tr_data = pd.DataFrame(tr_data)\n",
    "            tr_data['labels'] = pd.DataFrame(labels)\n",
    "            te_data, labels = test_data.tensors\n",
    "            te_data = te_data.squeeze()\n",
    "            te_data = pd.DataFrame(te_data)\n",
    "            te_data['labels'] = pd.DataFrame(labels)\n",
    "            X_train = tr_data.drop(columns='labels')\n",
    "            y_train = tr_data['labels']\n",
    "            X_test = te_data.drop(columns='labels')\n",
    "            y_test = te_data['labels']\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{label} Accuracy: {accuracy:.4f}\")\n",
    "            print('\\nClassification Report:')\n",
    "            print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_5844\\753908975.py:45: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  s1 = pd.read_csv('UNSW-NB15_1.csv', header=None)\n",
      "C:\\Users\\adi_s\\AppData\\Local\\Temp\\ipykernel_5844\\753908975.py:50: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  s2 = pd.read_csv('UNSW-NB15_2.csv', header=None)\n"
     ]
    }
   ],
   "source": [
    "labels = ['DoS', 'Fuzzers', 'Generic', 'Exploits', 'Analysis', 'Backdoor', 'Shellcode', 'Reconnaissance', 'Worms']\n",
    "be = BinaryEnsembler(labels, 128, 1, 20, 0.1, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda311new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
