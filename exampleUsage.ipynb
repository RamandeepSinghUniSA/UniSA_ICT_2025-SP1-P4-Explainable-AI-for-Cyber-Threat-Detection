{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations and arrays\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations\n",
    "import seaborn as sns  # For statistical data visualization\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn import metrics  # For model evaluation metrics\n",
    "from sklearn.model_selection import train_test_split  # For splitting datasets\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2540047\n",
      "2539744\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Cleaned_full_data.csv')\n",
    "print(len(df))\n",
    "df['attack_cat'] = df['attack_cat'].str.replace('Backdoors', 'Backdoor')\n",
    "# Interesting here. I was getting this error when encoding but you seemed to have fixed it.\n",
    "df = df[~df['sport'].astype(str).str.startswith('0x')]\n",
    "df = df[~df['dsport'].astype(str).str.startswith('0x')]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrEncoder:\n",
    "    \"\"\"\n",
    "    CorrEncoder: Takes a dataset as input and uses it for the encode function. Encodes the filtered categories then draws correlations.\n",
    "    If correlation is above the threshold adds it to a new dataframe then returns the one hot encoded values with the labels.\n",
    "\n",
    "    Initialisation:\n",
    "        - data (pd.DataFrame): The Dataset that contains the target column and target label variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data.copy()\n",
    "        # Removes Label for the multi-class processing as it is based on the label category (threat or not).\n",
    "        self.data = self.data.drop(columns=['attack_cat'])\n",
    "\n",
    "    def encode(self, target_column, sparse_n, threshold, print_data):\n",
    "        \"\"\"\n",
    "        encode: Takes a target column and target label to encode and draw correlations from. The target column is iterated through\n",
    "        for all categories that contain more positive values than defined in sparse_n. This allows for filtering of sparse categories.\n",
    "        The function then one hot encodes the given category with the static target column and draws correlations for them. If correlation\n",
    "        is greater then threshold then add it to the new DataFrame. The function returns the one hot encoded categories that pass the\n",
    "        threshold with the target label.\n",
    "\n",
    "        The purpose of this function is to resolve the high cardinality problem in one hot encoding.\n",
    "\n",
    "        Parameters:\n",
    "            - target_column (string): The name of the target column. The target column should contain the various categories to encode.\n",
    "            - sparse_n (integer): The minimum amount of positive values required for a category after encoding (deals with sparse categories).\n",
    "            - threshold (float): The threshold for correlation. The function creates onehot encoded columns of all variables that have correlation\n",
    "              higher that the threshold to the target label.\n",
    "\n",
    "        Returns:\n",
    "            - ohe_df (pd.DataFrame): The one hot encoded values from the target column.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data[target_column] = self.data[target_column].astype(str)\n",
    "        value_counts = self.data[target_column].value_counts()\n",
    "        # Check if number of 1s is above the given threshold set by sparse_n.\n",
    "        categories = value_counts[value_counts > sparse_n].index.tolist()\n",
    "        ohe_list = []\n",
    "        # Attack category for multi-class binary.\n",
    "        attack_cat = self.data['label']\n",
    "        \n",
    "        # Go through each unique category in the target column.\n",
    "        for c in categories:\n",
    "            col_name = f'{target_column}_{c}'\n",
    "\n",
    "            # Create the binary encoding column for the current category and target label.\n",
    "            corr_column = (self.data[target_column] == c).astype(int)\n",
    "            correlation = corr_column.corr(attack_cat)\n",
    "\n",
    "            # Check if absolute correlation is greater than threshold.\n",
    "            if abs(correlation) > threshold:\n",
    "                corr_column.name = col_name\n",
    "                ohe_list.append(corr_column)\n",
    "        if print_data:\n",
    "            print('Number of Encoded Features for', target_column)\n",
    "            print(len(ohe_list))\n",
    "        if ohe_list:\n",
    "            # NOTE: This section can be expanded to include print outs but at the moment am focusing on the evaluations.\n",
    "            ohe_df = pd.concat(ohe_list, axis=1)\n",
    "            return ohe_df\n",
    "        else:\n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2540047\n",
      "2539744\n"
     ]
    }
   ],
   "source": [
    "# Import cleaned data set. I get a hex string error in sport and dsport on my computer not sure if everyone else does too.\n",
    "df = pd.read_csv('Cleaned_full_data.csv')\n",
    "print(len(df))\n",
    "df['attack_cat'] = df['attack_cat'].str.replace('Backdoors', 'Backdoor')\n",
    "df = df[~df['sport'].astype(str).str.startswith('0x')]\n",
    "df = df[~df['dsport'].astype(str).str.startswith('0x')]\n",
    "df['ct_ftp_cmd'] = df['ct_ftp_cmd'].fillna(0)\n",
    "df = df.reset_index(drop=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataset to speed up process.\n",
    "df = sample_data = df.sample(n=1000, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encoded Features for dsport\n",
      "3\n",
      "Number of Encoded Features for proto\n",
      "3\n",
      "Number of Encoded Features for sport\n",
      "3\n",
      "Number of Encoded Features for srcip\n",
      "14\n",
      "Number of Encoded Features for dstip\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['state', 'service']\n",
    "# There is not many unique values here so it works ok.\n",
    "encoder = OneHotEncoder(sparse_output=False, dtype='float32')\n",
    "encoded_data = encoder.fit_transform(df[categorical_columns])\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns), index=df.index)\n",
    "full_encoded = pd.concat([df.drop(columns=categorical_columns), encoded_df], axis=1)\n",
    "\n",
    "# Select correlation threshold. All values with correlation less than threshold to the label are not encoded.\n",
    "# NOTE: Decreasing threshold 0.01 or lower significantly increases the number of columns. I think 0.01 was around 244 for downsampled but it usually is more for\n",
    "# full data. Set True to print the number of encoded columns thhat passed the check.\n",
    "threshold = 0.1\n",
    "encoder = CorrEncoder(full_encoded)\n",
    "ohe1 = encoder.encode('dsport', 30, threshold, True)\n",
    "ohe2 = encoder.encode('proto', 30, threshold, True)\n",
    "ohe3 = encoder.encode('sport', 30, threshold, True)\n",
    "ohe4 = encoder.encode('srcip', 30, threshold, True)\n",
    "ohe5 = encoder.encode('dstip', 30, threshold, True)\n",
    "cols_to_drop = ['dsport', 'proto', 'sport', 'srcip', 'dstip']\n",
    "filtered_data = full_encoded.drop(columns=cols_to_drop)\n",
    "combined_data = pd.concat([filtered_data, ohe1, ohe2, ohe3, ohe4, ohe5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              dur  sbytes  dbytes  sttl  dttl  sloss  dloss         sload  \\\n",
      "1410676  0.447346     568     312    31    29      0      0  7.618264e+03   \n",
      "2393246  0.004443     568     304    31    29      0      0  7.670492e+05   \n",
      "1061153  0.048964    4014   59366    31    29      7     28  6.461890e+05   \n",
      "1062617  0.305216    4888    2872    31    29      7      7  1.231914e+05   \n",
      "2301240  0.000007     264       0    60     0      0      0  1.508571e+08   \n",
      "\n",
      "                dload  spkts  ...  dstip_149.171.126.8  dstip_149.171.126.18  \\\n",
      "1410676  4.184680e+03      4  ...                    0                     0   \n",
      "2393246  4.105334e+05      4  ...                    0                     0   \n",
      "1061153  9.560983e+06     68  ...                    0                     0   \n",
      "1062617  7.239462e+04     26  ...                    0                     0   \n",
      "2301240  0.000000e+00      2  ...                    0                     0   \n",
      "\n",
      "         dstip_149.171.126.15  dstip_149.171.126.14  dstip_149.171.126.10  \\\n",
      "1410676                     0                     0                     0   \n",
      "2393246                     0                     0                     0   \n",
      "1061153                     0                     0                     0   \n",
      "1062617                     0                     0                     0   \n",
      "2301240                     0                     0                     0   \n",
      "\n",
      "         dstip_149.171.126.12  dstip_149.171.126.17  dstip_149.171.126.13  \\\n",
      "1410676                     0                     0                     0   \n",
      "2393246                     0                     0                     0   \n",
      "1061153                     0                     0                     0   \n",
      "1062617                     0                     0                     0   \n",
      "2301240                     0                     0                     0   \n",
      "\n",
      "         dstip_149.171.126.19  dstip_149.171.126.11  \n",
      "1410676                     0                     0  \n",
      "2393246                     0                     0  \n",
      "1061153                     0                     0  \n",
      "1062617                     0                     0  \n",
      "2301240                     0                     0  \n",
      "\n",
      "[5 rows x 110 columns]\n"
     ]
    }
   ],
   "source": [
    "# Verify data, here if the process fails for any reason (fixed index error), there will be null values.\n",
    "# NOTE: If the index is not reset then correlation is calculated based on the internal index and will only\n",
    "# compare the matching values.\n",
    "print(combined_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of Downsampling.\n",
    "- When looking at the difference between masked values and Normal values far from threats there did not seem to be significant differences. ltime and stime had some small differences. Many of the distributions are skewed so it is hard to analyse with just the mean. The mask can however be useful in the evaluation stage when using shap to group the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled Rows: 1078488\n"
     ]
    }
   ],
   "source": [
    "# Choose a random seed.\n",
    "rs = 42\n",
    "\n",
    "# Select proportion to downsample by.\n",
    "downsample = 0.5\n",
    "mask = (df['label'].shift(-1) != 1) & (df['label'].shift(1) != 1)\n",
    "normal_rows = df[(df['attack_cat'] == 'Normal') & mask]\n",
    "percentage_to_remove = int(len(normal_rows) * downsample)\n",
    "rows_to_remove = normal_rows.sample(n=percentage_to_remove, random_state=rs)\n",
    "df = df.drop(rows_to_remove.index)\n",
    "print(f\"Downsampled Rows: {len(rows_to_remove)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda311new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
